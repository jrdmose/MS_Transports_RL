# Simulations

All simulations have been done in a rush hour scenario

## State definition with wt and wo.

  State with wt shows better converngence than wo: Loss is significantly lower and reaches stable level before.
  Is the case for the simple network and the ?comple as well?

  Results stored in  logs/Simple_W_reward_WT, logs/Simple_WO_reward_WT, logs/Complex_W_reward_WT,


## Policy eps vs linDecEpsGreedy

  Seems that epsgreedy performs better than linDecEpsGreedy. LinDecEps takes longer to converge and when it converges it Does
  not converge to better results than LinDEc.

  ALso, epsGreedy with lower eps is more stable than high eps. Hoewever, it seems thatn when evaluating it does as good as with
  lower eps.

  Results stored in  logs/Simple_W_reward_WT, logs/Simple_WO_reward_WT, logs/Complex_W_reward_WT,

  Try lower eps. Try lindec with higer decreasing rate.

  - Lower eps gives good results (0.05, 0.1) improving fixed policy by 20 s!!!
    Results stored in old_logs/Simple_Balanced_Negative_Reward/

## Balanced vs negative rewards


  - Simple setup
    Simulations show a worse performance during training and evaluating for negative reward not managing to beat the fixed policy.
    For Balanced reward we obtain very good results. Particularly good was run 9,3,11 int training and evaluation

    However, reward plot seem to have a better evolution since they increase (they are always egative) until they converge.

    Results stored in old_logs/Simple_Balanced_Negative_Reward/

  - Complex setup
    ?

## Different length of relevant lanes

- Simple
  Big lanes seem much more volatile in average_delay than a shorter lane area. Particularly bad big with balanced reward with 5/5 nothing
  finished in evaluation.

  Particularly interesting is run5 in Simple which fails to finish even having good training curves. The problem is the last car gets stucked
  and the light does not open. (SImilarly as what we detected in the linear case) I was not able to reproduce this. Probably there is something
  wwrong with seeds.

  Overall, impression of volatility for longer influence area and evaluate results higher with higher number of runs not finished. See word for RESULTS

  Selected runs: Simple run 7, 4.

  / Rsults stored in "/Experiment_results/Simple_Balanced_Negative_Reward/ and /Experiment_results/Big_Simple_Balanced_Negative_Reward/

  - Complex
