{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "##########################\n",
    "\n",
    "import agent\n",
    "import environment\n",
    "import doubledqn\n",
    "import tools\n",
    "import memory\n",
    "import simulation\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools\n",
    "from keras import optimizers \n",
    "\n",
    "def iter_params(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in itertools.product(*vals):\n",
    "        yield dict(zip(keys, instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONE RUN\n",
    "#############################################\n",
    "\n",
    "param = { \n",
    "    \n",
    "    \"batch_size\" : 70, # 30,50,60,[70]\n",
    "    \"target_update_freq\" : 1000,#10000], # 10000,20000,30000,[40000]\n",
    "    \"gamma\" : 0.99,# # 0.98,0.99,0.995,[0.999]\n",
    "    \"train_freq\" : 1, # 2,3,4,[5]\n",
    "    \"max_size\" : 100000,#,70000], # 20000,50000,70000,[100000]\n",
    "    \"max_ep_length\" : 2000, # 1000,2000,3000,[4000]\n",
    "    \"policy\" : \"epsGreedy\",#, \"epsGreedy\"],\n",
    "    \"eps\" : 0.1,#, 0.3, 0.1],\n",
    "    \"delta_time\" : 10,\n",
    "    #\"optimizer\": [optimizers.RMSprop(lr= 0.001), optimizers.Adagrad(), optimizers.Adam()]\n",
    "}\n",
    "\n",
    "sumo_RL = simulation.simulator(\n",
    "                    connection_label = \"single_worker\",\n",
    "                    q_network_type = 'simple',\n",
    "                    target_q_network_type = 'simple',\n",
    "                    gamma = param[\"gamma\"],\n",
    "                    target_update_freq = param[\"target_update_freq\"],\n",
    "                    train_freq = param[\"train_freq\"],\n",
    "                    num_burn_in = 200,\n",
    "                    batch_size = param[\"batch_size\"],\n",
    "                    optimizer = 'adam',\n",
    "                    loss_func = \"mse\",\n",
    "                    max_ep_length = param[\"max_ep_length\"],\n",
    "                    experiment_id = \"Testing_Loading\",\n",
    "                    model_checkpoint = True,\n",
    "                    opt_metric = None,\n",
    "\n",
    "                   # environment parameters\n",
    "                    net_file = \"cross.net.xml\",\n",
    "                    route_file = \"cross.rou.xml\",\n",
    "                    demand = \"nominal\",\n",
    "                    state_shape = (1,19),\n",
    "                    num_actions = 2,\n",
    "                    use_gui = True,\n",
    "                    delta_time = param[\"delta_time\"],\n",
    "\n",
    "                   # memory parameters\n",
    "                    max_size = param[\"max_size\"],\n",
    "\n",
    "                   # additional parameters\n",
    "\n",
    "                    policy = param[\"policy\"],\n",
    "                    eps = param[\"eps\"],\n",
    "                    num_episodes = 2,\n",
    "                    monitoring = True,\n",
    "                    episode_recording = False,\n",
    "                    hparams = param.keys())\n",
    "\n",
    "sumo_RL.train()\n",
    "# agent.load(\"./logs/First gridsearch/run_144/model_checkpoints/runFirst gridsearch_iter165000.h5\")\n",
    "\n",
    "# agent.ddqn.train(env = agent.env, num_episodes = 100, policy = agent.policy, connection_label = agent.connection_label)\n",
    "\n",
    "#evaluation_results = agent.evaluate(runs=2, use_gui= False)\n",
    "#evaluation_results\n",
    "# data= agent.ddqn.evaluate(env = agent.env, policy = \"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started process # 1\n",
      "Started process # 2\n",
      "Started process # 3\n",
      "Started process # 4\n",
      "Started process # 5\n",
      "Started process # 6\n",
      "Started process # 7\n",
      "Started process # 8\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      "...done filling replay memory\n",
      "Run 2 -- running episode 1 / 2\n",
      "...done filling replay memory\n",
      "Run 1 -- running episode 1 / 2\n",
      " Retrying in 1 seconds\n",
      " Retrying in 1 seconds\n",
      "Run 1 -- running episode 2 / 2\n",
      " Retrying in 1 seconds\n",
      "Run 2 -- running episode 2 / 2\n",
      " Retrying in 1 seconds\n",
      "Evaluate 1 -- running episode 1 / 5\n",
      " Retrying in 1 seconds\n",
      "Evaluate 2 -- running episode 1 / 5\n",
      " Retrying in 1 seconds\n",
      "Evaluate 2 -- running episode 2 / 5\n",
      " Retrying in 1 seconds\n",
      "Evaluate 1 -- running episode 2 / 5\n",
      " Retrying in 1 seconds\n",
      "Evaluate 2 -- running episode 3 / 5\n",
      " Retrying in 1 seconds\n",
      "Evaluate 1 -- running episode 3 / 5\n",
      " Retrying in 1 seconds\n",
      "Evaluate 2 -- running episode 4 / 5\n",
      " Retrying in 1 seconds\n",
      "Evaluate 1 -- running episode 4 / 5\n",
      " Retrying in 1 seconds\n",
      "Evaluate 2 -- running episode 5 / 5\n",
      " Retrying in 1 seconds\n",
      "Evaluate 1 -- running episode 5 / 5\n",
      " Retrying in 1 seconds\n"
     ]
    }
   ],
   "source": [
    "# GRIDSEARCH\n",
    "###############################################################\n",
    "\n",
    "experiment_id = \"new_reward2\" \n",
    "\n",
    "\n",
    "param = { \n",
    "    \n",
    "    \"batch_size\" : [50], # 30,50,60,[70]\n",
    "    \"target_update_freq\" : [20000],  #10000], # 10000,20000,30000,[40000]\n",
    "    \"gamma\" : [0.99],# # 0.98,0.99,0.995,[0.999]\n",
    "    \"train_freq\" : [1], # 2,3,4,[5]\n",
    "    \"max_size\" : [100000],#,70000], # 20000,50000,70000,[100000]\n",
    "    \"max_ep_length\" : [2000], # 1000,2000,3000,[4000]\n",
    "    \"policy\" : [\"epsGreedy\", \"epsGreedy\", \"linDecEpsGreedy\"],#, \"epsGreedy\"],\n",
    "    \"eps\" : [0.2, 0.5],#, 0.3, 0.1],\n",
    "    \"delta_time\" : [10,20],\n",
    "    \"reward\" : [\"balanced\", \"negative\"]\n",
    "    #\"optimizer\": [optimizers.RMSprop(lr= 0.001), optimizers.Adagrad(), optimizers.Adam()]\n",
    "}\n",
    "\n",
    "param_grid = iter_params(**param)\n",
    "\n",
    "\n",
    "\n",
    "def worker(input, output):\n",
    "    \"\"\"Runs through a chunk of the grid\"\"\"\n",
    "\n",
    "    for position, args in iter(input.get, 'STOP'):\n",
    "        result = worker_task(position, args)\n",
    "        output.put(result)\n",
    "\n",
    "\n",
    "def worker_task(position, args):\n",
    "    \"\"\"Tells the worker what to do with grid chunk\"\"\"\n",
    "    # initialise all obje\n",
    "    \n",
    "    # print('Run', position + 1, '-- parameters', args)\n",
    "    \n",
    "    sumo_RL = simulation.simulator(\n",
    "                    connection_label = position + 1,\n",
    "                    q_network_type = 'simple',\n",
    "                    target_q_network_type = 'simple',\n",
    "                    gamma = args[\"gamma\"],\n",
    "                    target_update_freq = args[\"target_update_freq\"],\n",
    "                    train_freq = args[\"train_freq\"],\n",
    "                    num_burn_in = 1000,\n",
    "                    batch_size = args[\"batch_size\"],\n",
    "                    optimizer = 'adam',\n",
    "                    loss_func = \"mse\",\n",
    "                    max_ep_length = args[\"max_ep_length\"],\n",
    "                    experiment_id = experiment_id,\n",
    "                    model_checkpoint = True,\n",
    "                    opt_metric = None,\n",
    "\n",
    "                   # environment parameters\n",
    "                    net_file = \"cross.net.xml\",\n",
    "                    route_file = \"cross.rou.xml\",\n",
    "                    demand = \"nominal\",\n",
    "                    state_shape = (1,19),\n",
    "                    num_actions = 2,\n",
    "                    use_gui = False,\n",
    "                    delta_time = args[\"delta_time\"],\n",
    "                    reward = args[\"reward\"],\n",
    "\n",
    "                   # memory parameters\n",
    "                    max_size = args[\"max_size\"],\n",
    "\n",
    "                   # additional parameters\n",
    "\n",
    "                    policy = args[\"policy\"],\n",
    "                    eps = args[\"eps\"],\n",
    "                    num_episodes = 1000,\n",
    "                    monitoring = True,\n",
    "                    episode_recording = False,\n",
    "                    hparams = args.keys())\n",
    "    \n",
    "    # print(\"training agent\", position + 1)\n",
    "    sumo_RL.train()\n",
    "    # print(\"evaluating agent\", position + 1)\n",
    "    evaluation_results = sumo_RL.evaluate(runs = 5)\n",
    "\n",
    "    return (position + 1, args, evaluation_results)\n",
    "\n",
    "\n",
    "def gridsearch(param_grid):\n",
    "    \"\"\"Runs a parallelised gridsearch\"\"\"\n",
    "\n",
    "    number_of_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # Set up task list\n",
    "    tasks = [(idx, val) for idx, val in enumerate(param_grid)]\n",
    "\n",
    "    # Create queues\n",
    "    task_queue = multiprocessing.Queue()\n",
    "    done_queue = multiprocessing.Queue()\n",
    "\n",
    "    # Submit tasks\n",
    "    for task in tasks:\n",
    "        task_queue.put(task)\n",
    "    # Start worker processes\n",
    "    for i in range(number_of_processes):\n",
    "        print('Started process #', i + 1)\n",
    "        multiprocessing.Process(target = worker,\n",
    "                                args = (task_queue, done_queue)).start()\n",
    "        \n",
    "#     with open(os.path.join(\"./logs\",experiment_id,\"GS_results\"), \"w\",newline='') as file:\n",
    "#             writer = csv.writer(file, dialect = 'excel')\n",
    "#             writer.writerow(experiment_id)                \n",
    "      \n",
    "    # Get and print results\n",
    "    results = []\n",
    "    for i in range(len(tasks)):\n",
    "        results.append(done_queue.get())\n",
    "        with open(os.path.join(\"./logs\",experiment_id,\"GS_results.json\"), \"a\") as file:\n",
    "            json.dump(results[-1], file , indent=4)\n",
    "                  \n",
    "        #print('%s -- [RESULTS]: Run %s -- Parameters %s -- Mean duration %6.0f' % results[-1])\n",
    "        \n",
    "    # Tell child processes to stop\n",
    "    for i in range(number_of_processes):\n",
    "        task_queue.put('STOP')\n",
    "\n",
    "    # Now combine the results\n",
    "    scores = [result[-1] for result in results]\n",
    "    lowest = min(scores)\n",
    "    winner = results[scores.index(lowest)]\n",
    "    return winner, results\n",
    "\n",
    "multiprocessing.freeze_support()\n",
    "winner, results = gridsearch(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-100f62972f2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_test = []\n",
    "eps_test = 0.8\n",
    "for i in range(300000):\n",
    "    eps_test *= omega_test ** i\n",
    "    decay_test.append(eps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(300000),decay_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_results = pd.DataFrame(results).sort_values(by = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
