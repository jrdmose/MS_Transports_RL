{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "##########################\n",
    "\n",
    "import random\n",
    "import copy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "import os, sys\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Making sure path to SUMO bins correctly specified\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "import sumolib\n",
    "import traci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# OUTPUT FILE FOLDER\n",
    "####################################\n",
    "\n",
    "def get_output_folder(parent_dir, exp_id):\n",
    "    \"\"\"Return save folder parent_dir/Results/exp_id\n",
    "    \n",
    "    If this directory already exists it creates parent_dir/Results/exp_id_{i}, \n",
    "    being i the next smallest free number.\n",
    "    \n",
    "    Inside this directory it also creates a sub-directory called model_checkpoints to \n",
    "    store intermediate training steps.\n",
    "    \n",
    "    This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir : str\n",
    "        Path of the directory where results will be stored.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/Results/exp_id\n",
    "        Path to this run's save directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Returns an error if parent_dir already exists\n",
    "        os.makedirs(parent_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if exp_id in os.listdir(parent_dir):\n",
    "        \n",
    "        experiment_id = 1\n",
    "        new_folder = os.path.join(parent_dir,exp_id+\"_\"+str(experiment_id))\n",
    "        \n",
    "        while os.path.exists(new_folder):\n",
    "            experiment_id +=1\n",
    "            new_folder = os.path.join(parent_dir,exp_id+\"_\"+str(experiment_id))\n",
    "        \n",
    "        parent_dir = new_folder\n",
    "        os.makedirs(parent_dir)\n",
    "        os.mkdir(os.path.join(parent_dir,\"model_checkpoints\"))\n",
    "    else:\n",
    "        parent_dir = os.path.join(parent_dir,exp_id)\n",
    "        os.makedirs(parent_dir)\n",
    "        os.mkdir(os.path.join(parent_dir,\"model_checkpoints\"))\n",
    "        \n",
    "    return parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MEMORY CLASS\n",
    "##################################################\n",
    "\n",
    "class ReplayMemory():\n",
    "    \"\"\" \n",
    "    Keeps a memory of transitions the environment has revealed as response to actions taken.\n",
    "    \n",
    "    Keeps a memory of size max_size, implemented through lists of class SingleSample: state, action, reward and nextstate.\n",
    "    An index counter is pushed forward, through the list as the memory is filled. Once memory is full\n",
    "    the index starts from 0, and overwrites existing memory. The index counter is also used to indicate\n",
    "    which one the most recent memory is, to allow prioritising more recent memory over older memory.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    memory: (list) list of class SingleSample containing memory of max_size of transitions\n",
    "    max_size : (int) memory capacity required\n",
    "    itr : (int) current index\n",
    "    cur_size : (int) current size of memory\n",
    "    \n",
    "    Methods\n",
    "    ------\n",
    "    append(state, action, reward, nextstate)\n",
    "        adds new elements to the four lists, handles index counter\n",
    "        \n",
    "    sample(batch_size)\n",
    "        Randomly draws a sample of batch_size of transitions. It returns 2 arrays and two lists.\n",
    "        The arrays correspond to state and next state of the transitions with dimensions \n",
    "        (batch_size, space_shape).The lists correspond to actions and rewards of those transitions.\n",
    "        \n",
    "    print_obs(obs)\n",
    "        prints a specific transition\n",
    "        \n",
    "    get_size\n",
    "        returns the current size of the memory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size, state_shape, num_actions):\n",
    "        \"\"\"Initialize the whole memory at once.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        max_size : (int) memory capacity required\n",
    "        state_shape : (tuple) tuple specifying the shape of the array in which state variables are stored.\n",
    "        num_actions : (int) number of actions (traffic signal phases)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory = [SingleSample(state_shape,num_actions) for _ in range(max_size)]\n",
    "        self.max_size = max_size\n",
    "        self.itr = 0  # insert the next element here\n",
    "        self.cur_size = 0\n",
    "    \n",
    "    \n",
    "    def append(self, state, action, reward, nextstate):\n",
    "        \"\"\"Adds new elements to the four lists, handles index counter.\n",
    "                \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : (np.array) all state variables for one observation\n",
    "        action : (int) index of the one-hot encoded vector indicating action\n",
    "        reward : (float) reward after action taken\n",
    "        nextstate : (np.array) all state variables for one observation, after action taken\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory[self.itr].assign(state, action, reward, nextstate)\n",
    "        self.itr += 1\n",
    "        self.cur_size = min(self.cur_size + 1, self.max_size)\n",
    "        self.itr %= self.max_size\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Uniform sampling, later prioritized experience replay can be implemented.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : (int) size of the batch to be sampled\n",
    "        \"\"\"\n",
    "\n",
    "        states, actions, rewards, next_states = [],[],[],[]\n",
    "        for i, idx in enumerate(np.random.randint(0, self.cur_size, size=batch_size)):\n",
    "            transition = self.memory[idx]\n",
    "            states.append(transition.state)\n",
    "            actions.append(transition.action)\n",
    "            rewards.append(transition.reward)\n",
    "            next_states.append(transition.nextstate)\n",
    "        return np.vstack(states), actions, rewards, np.vstack(next_states)\n",
    "    \n",
    "    \n",
    "    def print_obs(self,obs):\n",
    "        \"\"\"Selects a specific transition to view.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : (int) index of the specific transition to view\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory[obs].print_obs() # This calls a SingleSample method called also print_obs\n",
    "\n",
    "        \n",
    "    def get_size(self):\n",
    "        \"\"\"Shows current size of memory\"\"\"\n",
    "        \n",
    "        return self.cur_size\n",
    "    \n",
    "    \n",
    "    \n",
    "class SingleSample():\n",
    "    \"\"\"A helper for the memory class. It stores single transition objects.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    state : (np.array) all state variables for one observation\n",
    "    action : (int) index of the one-hot encoded vector indicating action\n",
    "    reward : (int) reward after action taken\n",
    "    nextstate : (np.array) all state variables for one observation, after action taken\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    assign(self, state, action, reward, nextstate)\n",
    "        Assigns new values to attributes.\n",
    "        \n",
    "    print_obs()\n",
    "        Prints current observation/ transition.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_shape, num_actions): # Num actions not used up to now\n",
    "        \"\"\"Initialises object instance.\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_shape : (tuple) (tuple) tuple specifying the shape of the array in which state variables are stored.\n",
    "        num_actions : (int) number of actions (traffic signal phases)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state = np.zeros(state_shape)\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        self.nextstate = np.zeros(state_shape)\n",
    "        \n",
    "        \n",
    "    def assign(self, state, action, reward, nextstate):\n",
    "        \"\"\"Assigns new values to attributes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : (np.array) all state variables for one observation\n",
    "        action : (np.array) index of the one-hot encoded vector indicating action\n",
    "        reward : (int) reward after action taken\n",
    "        nextstate : (np.array) all state variables for one observation, after action taken\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state[:] = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.nextstate[:] = nextstate\n",
    "        \n",
    "        \n",
    "    def print_obs(self):\n",
    "        \"\"\"Prints current observation\"\"\"\n",
    "        \n",
    "        print( \"State: \\n\\n\",self.state,\n",
    "               \"\\n\\nAction:\\n\\n\",self.action,\n",
    "               \"\\n\\nReward:\\n\\n\",self.reward,\n",
    "               \"\\n\\nNext State:\\n\\n\",self.nextstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ROUTING (DEMAND)\n",
    "#################################################\n",
    "\n",
    "### TO DO\n",
    "# Create a class to specify different types of demand\n",
    "\n",
    "def generate_routefile():\n",
    "    \"\"\"Returns XML file specifying network layout for sumo simulation\"\"\"\n",
    "    random.seed(42)  # make tests reproducible\n",
    "    N = 3600  # number of time steps\n",
    "    # demand per second from different directions\n",
    "\n",
    "    pEW = 1 / 20\n",
    "    pNS = 1 / 80\n",
    "    pWE = 1 / 20\n",
    "    pSN = 1 / 80\n",
    "\n",
    "    with open(\"cross.rou.xml\", \"w\") as routes:\n",
    "        print(\"\"\"<routes>\n",
    "        <vType id=\"car\" accel=\"0.8\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"16.67\" guiShape=\"passenger\"/>\n",
    "        <route id=\"right\" edges=\"51o 1i 2o 52i\" />\n",
    "        <route id=\"left\" edges=\"52o 2i 1o 51i\" />\n",
    "        <route id=\"down\" edges=\"54o 4i 3o 53i\" />\n",
    "        <route id=\"up\" edges=\"53o 3i 4o 54i\" />\"\"\", file=routes)\n",
    "        vehNr = 0\n",
    "        for i in range(N):\n",
    "            if random.uniform(0, 1) < pWE:\n",
    "                print('    <vehicle id=\"right_%i\" type=\"car\" route=\"right\" depart=\"%i\" />' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pEW:\n",
    "                print('    <vehicle id=\"left_%i\" type=\"car\" route=\"left\" depart=\"%i\" />' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pNS:\n",
    "                print('    <vehicle id=\"down_%i\" type=\"car\" route=\"up\" depart=\"%i\" color=\"1,0,0\"/>' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pSN:\n",
    "                print('    <vehicle id=\"UP_%i\" type=\"car\" route=\"down\" depart=\"%i\" color=\"1,0,0\"/>' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "        print(\"</routes>\", file=routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Q NETWORKS\n",
    "################################\n",
    "\n",
    "# More ANN arquitectures to be specified here\n",
    "def get_model(model_name, *args):\n",
    "    \"\"\"Helper function to instantiate q-networks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : (str) Name of the network architecture to be used to instantiate q-network\n",
    "    *args : arguments to be passed onto helper functions\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name == 'linear':\n",
    "        return linear(*args)\n",
    "    elif model_name == 'simple':\n",
    "        return simple(*args)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "        \n",
    "def linear(input_shape, num_actions):\n",
    "    \"\"\"Feeds into get_model. Sets up a linear keras model instance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : (np.array) shape of the state vector to be fed into the model as input layer\n",
    "    num_actions : (int) number of nodes of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape)) # If a vector the flattening does not have any effect. (only matrices)\n",
    "    model.add(Dense(num_actions,activation=None))\n",
    "    return model\n",
    "\n",
    "\n",
    "def simple(input_shape, num_actions):\n",
    "    \"\"\"Feeds into get_model. Sets up a neural network keras model instance.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : (np.array) shape of the state vector to be fed into the model as input layer\n",
    "    num_actions : (int) number of nodes of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, input_shape = input_shape, activation='relu'))\n",
    "    model.add(Dense(9, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation=None))\n",
    "    return model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ENVIRONMENT CLASS\n",
    "##################################   \n",
    "class Env:\n",
    "    \"\"\"Main class to manage environment. Supplies the environment responses to actions taken.\n",
    "    \n",
    "    Sends commands to sumo to take an action, receives state information,\n",
    "    computes rewards. Does so step by step to allow q-network to train batch-wise.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    net : (str) points to the SUMO .net.xml file which specifies network arquitechture\n",
    "    route : (str) points to the SUMO .rou.xml file which specified traffic demand\n",
    "    use_gui : (bool) Whether to run SUMO simulation with GUI visualisation\n",
    "    time_step : (int) Simulation seconds between actions\n",
    "    sumo_binary : (str) Points to the binary to run sumo\n",
    "    num_actions : num_actions : (int) number of actions (traffic signal phases)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    start_simulation()\n",
    "        Opens call to sumo\n",
    "    \n",
    "    take_action(action)\n",
    "        Sets the traffic lights according to the action fed as argument\n",
    "\n",
    "    compute_reward(state, next_state)\n",
    "        Takes the current state and next state (from object state) and computes reward\n",
    "\n",
    "    step(action)\n",
    "        Combines take_action, compute_reward and update_state (from observation object)\n",
    "\n",
    "    done()\n",
    "        checks whether all links are empty, and hence the simulation done\n",
    "\n",
    "    stop_simulation()\n",
    "        closes the sumo/traci call\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 net_file, \n",
    "                 route_file,\n",
    "                 state_shape,\n",
    "                 num_actions,\n",
    "                 use_gui = False,\n",
    "                 delta_time=10):\n",
    "        \"\"\"Initialises object instance.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        net_file : (str) SUMO .net.xml file\n",
    "        route_file : (str) SUMO .rou.xml file\n",
    "        state_shape : (np.array) 2-dimensional array specifying state space dimensions\n",
    "        num_actions : (int) specifying the number of actions available\n",
    "        use_gui : (bool) Whether to run SUMO simulation with GUI visualisation\n",
    "        delta_time : (int) Simulation seconds between actions\n",
    "        \"\"\"\n",
    "        \n",
    "        self.net = net_file\n",
    "        self.route = route_file\n",
    "        self.use_gui = use_gui\n",
    "        self.time_step = delta_time\n",
    "        if self.use_gui:\n",
    "            self.sumo_binary = sumolib.checkBinary('sumo-gui')\n",
    "        else:\n",
    "            self.sumo_binary = sumolib.checkBinary('sumo')\n",
    "            \n",
    "        self.obs = Observation(state_shape)\n",
    "        self.action = Action(num_actions)\n",
    " \n",
    "\n",
    "    def start_simulation(self):\n",
    "        \"\"\"Opens a connection to sumo/traci [with or without GUI] and \n",
    "        updates obs atribute  (the current state of the environment).\n",
    "        \"\"\"\n",
    "    \n",
    "        sumo_cmd = [self.sumo_binary, \n",
    "                    '-n', self.net,\n",
    "                    '-r' ,self.route]\n",
    "        \n",
    "        if self.use_gui:\n",
    "            sumo_cmd.append('--start')\n",
    "        traci.start(sumo_cmd)\n",
    "        self.obs.update_state()\n",
    "        \n",
    "        \n",
    "    def take_action(self,action):\n",
    "        \"\"\"Sets the action variable in sumo/traci to a new value.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : (int) index of the one-hot encoded vector of action to be taken\n",
    "        \"\"\"\n",
    "        \n",
    "        #action = 0 -> row vertically\n",
    "        if action == 0 and traci.trafficlight.getPhase(\"0\") != 0:\n",
    "            traci.trafficlight.setPhase(\"0\",3)\n",
    "        #action = 1 -> row horizontally    \n",
    "        elif action == 1 and traci.trafficlight.getPhase(\"0\") != 2:\n",
    "            traci.trafficlight.setPhase(\"0\",1)\n",
    "               \n",
    "                \n",
    "    def compute_reward(self, state, next_state):\n",
    "        \"\"\" Computes reward from state and next_state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : (np.array) vector of current state\n",
    "        next_state: (np.array) vector of next state\n",
    "        \"\"\"\n",
    "            \n",
    "        # Here is whre reward is specified\n",
    "        a = next_state - state\n",
    "        b = np.round(state,decimals=1)\n",
    "\n",
    "        aux = np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "        \n",
    "        return -np.sum(aux[0,0:4]) #-np.sum(next_state[0,:4])#np.sum(a[0,4:-1])-np.sum(a[0,:4]) # - delta number of stopped cars\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\" Runs one step of the simulation.\n",
    "        \n",
    "        Makes a deep copy of the current state,\n",
    "        runs the simulation with the currently set action,\n",
    "        gets the state after the action,\n",
    "        and computes the reward associated to action taken.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : (int) index of the one-hot encoded vector of action to be taken\n",
    "        \"\"\"\n",
    "        \n",
    "        state = copy.deepcopy(self.obs.get())\n",
    "        \n",
    "        if action != None:\n",
    "            self.take_action(action)\n",
    "        \n",
    "        traci.simulationStep(traci.simulation.getTime() + self.time_step) # Run the simulation time_step (s)\n",
    "        \n",
    "        self.obs.update_state()\n",
    "        next_state = self.obs.get()\n",
    "        \n",
    "        reward = self.compute_reward(state,next_state)\n",
    "        \n",
    "        return state, reward, next_state, self.done()\n",
    "    \n",
    "    \n",
    "    def done(self):\n",
    "        \"\"\"Calls sumo/traci to check whether there are still cars in the network\"\"\"\n",
    "        \n",
    "        return traci.simulation.getMinExpectedNumber() == 0\n",
    "    \n",
    "    \n",
    "    def stop_simulation(self):\n",
    "        \"\"\"Closes the sumo/traci connection\"\"\"\n",
    "        \n",
    "        traci.close()\n",
    "        \n",
    "\n",
    "    \n",
    "class Observation:\n",
    "    \"\"\"\n",
    "    Helper class for environment. Handles the updating of the state. Ports the state from sumo/traci to python.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    obs : (np.array) holds the state variables\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    update_state() \n",
    "        reads the state through a call to sumo/traci\n",
    "            --> modify here to include additional state variables, \n",
    "            but make sure to also modify state_shape when running the simulation!\n",
    "            \n",
    "    get()\n",
    "        returns state\n",
    "        \n",
    "    get_reward()\n",
    "        returns reward, computed from current state\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : (tuple) specifying dimensionality of state vector\n",
    "        \"\"\"\n",
    "        \n",
    "        self.obs = np.zeros(shape)\n",
    "        \n",
    "        \n",
    "    def update_state(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        lanes : (list) hardcoded list of lanes in network\n",
    "        # TODO: read this from network file\n",
    "        \"\"\"\n",
    "        \n",
    "        lanes = [\"4i_0\",\"2i_0\",\"3i_0\",\"1i_0\"]           \n",
    "        for i,lane in enumerate(lanes):\n",
    "\n",
    "            self.obs[:,i] = traci.lane.getLastStepHaltingNumber(lane)\n",
    "            self.obs[:,i+4] = traci.lane.getLastStepMeanSpeed(lane)\n",
    "        \n",
    "        self.obs[:,8] = traci.trafficlight.getPhase(\"0\")\n",
    "        \n",
    "        \n",
    "    def get(self):\n",
    "        \"\"\"Returns state vector\"\"\"\n",
    "        \n",
    "        return self.obs\n",
    "        \n",
    "        \n",
    "#     def get_reward(self):\n",
    "#         \"\"\"compute reward corresponding to current state\"\"\"\n",
    "        \n",
    "#         return -np.sum(self.obs[0:4])\n",
    "    \n",
    "    \n",
    "    \n",
    "class Action:\n",
    "    \"\"\"\n",
    "    Helper class for observation. One-hot encoding of the phase of the traffic signal. \n",
    "    The methods for this class handle the selection of the best action.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    select_action(policy, **kwargs)\n",
    "        Takes policy as argument, and then calls the corresponding method.\n",
    "        # TODO: assert that the kwargs being fed correspond to the policy selected,\n",
    "        and handle errors\n",
    "\n",
    "    select_rand()\n",
    "        Select one of the actions randomly.\n",
    "    \n",
    "    select_greedy(q-values)\n",
    "        Check which action corresponds to the highest predicted reward/q-value.\n",
    "    \n",
    "    select_epsgreedy(eps, q_values)\n",
    "        Choose whether to explore or exploit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, num_actions):\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.action_space = np.identity(num_actions)\n",
    "        \n",
    "        \n",
    "    def select_action(self, policy, **kwargs):\n",
    "        \"\"\"Takes policy as argument, and then calls the corresponding helper method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        policy : (string) indicating which policy to consider. \n",
    "            Currently implemented: \n",
    "                - Pick action randomly (\"rand\")\n",
    "                - Pick action greedely (\"greedy\")\n",
    "                - Pick action in a eps - greedy fashion (\"epsGreedy\")\n",
    "                                    \n",
    "        **kwargs : arguments for helper methods\n",
    "        \"\"\"\n",
    "        \n",
    "        if policy == \"randUni\":\n",
    "            return self.select_rand()\n",
    "        elif policy == \"greedy\":\n",
    "            return self.select_greedy(**kwargs)\n",
    "        elif policy == \"epsGreedy\":\n",
    "            return self.select_epsgreedy(**kwargs)\n",
    "    \n",
    "    \n",
    "    def select_rand(self):\n",
    "        \"\"\"Feeds into select_greedy or directly into select_action method.\n",
    "        Selects one of the actions randomly.\"\"\"\n",
    "        \n",
    "        return np.random.randint(0, self.num_actions)\n",
    "    \n",
    "    \n",
    "    def select_greedy(self, q_values):\n",
    "        \"\"\"Feeds into select_epsgreedy or directly into select_action method.\n",
    "        Checks which action corresponds to the highest predicted reward/q-value.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        q_values : (np.array) predicted q-values\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    \n",
    "    def select_epsgreedy(self, eps, q_values):\n",
    "        \"\"\"Feeds into select_action method.\n",
    "        If explore, select action randomly,\n",
    "        if exploit, select action greedily using the predicted q values\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        eps : (int) exploration paramter\n",
    "        q_values : (np.array) predicted q-values\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.random.uniform() < eps:\n",
    "            return self.select_rand()\n",
    "        else:\n",
    "            return self.select_greedy(q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# DOUBLE DQN\n",
    "################################\n",
    "\n",
    "SAVE_AFTER = 11000 # Save model checkpoint\n",
    "\n",
    "class DoubleDQN:\n",
    "    \"\"\"The DQN agent. Handles the updating of q-networks, takes action, and gets environment response.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    q_network : keras model instance to predict q-values for current state\n",
    "    target_q_network : keras model instance to predict q-values for state after action\n",
    "    memory : memory instance - needs to be instantiated first # should this be instantiated here?\n",
    "    gamma : (int) discount factor for rewards\n",
    "    target_update_freq : (int) defines after how many steps the q-network should be re-trained\n",
    "    num_burn_in : (int) defines the size of the replay memory to be filled before, using a specified policy \n",
    "    batch_size : (int) size of batches to be used to train models\n",
    "    trained_episodes : (int) episode counter\n",
    "    max_ep_len : (int) stops simulation after specified number of episodes\n",
    "    output_dir : (str) directory to write tensorboard log and model checkpoints\n",
    "    experiment_id : (str) ID of simulation\n",
    "    summary_writer : tensorboard summary stat writer instance\n",
    "    itr : (int) counts global training steps in all run episodes\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    __compile()\n",
    "        Initialisation method, using the keras instance compile method.\n",
    "        \n",
    "    fill_replay()\n",
    "        Helper method for train. Fills the memory before model training begins.\n",
    "        \n",
    "    predict_q()\n",
    "        Helper method for train. Calls keras predict function using the keras model instance.\n",
    "    \n",
    "    save()\n",
    "        Calls keras save method using the keras model instance.\n",
    "    \n",
    "    update_network(env, policy)\n",
    "        Helper method for train. Computes keras neural network updates using samples from memory.\n",
    "        \n",
    "    train(env, num_episodes, policy, **kwargs)\n",
    "        Main method for the agent. Trains the keras neural network instances, calls all other helper methods.\n",
    "    \n",
    "    evaluate(env)\n",
    "        Use trained agent to run a simulation without training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 q_network, \n",
    "                 target_q_network,\n",
    "                 memory,\n",
    "                 gamma, \n",
    "                 target_update_freq,\n",
    "                 num_burn_in,\n",
    "                 batch_size,\n",
    "                 optimizer,\n",
    "                 loss_func,\n",
    "                 max_ep_length,\n",
    "                 env_name,\n",
    "                 output_dir,\n",
    "                 experiment_id,\n",
    "                 summary_writer,\n",
    "                 model_checkpoint = True,\n",
    "                 opt_metric = None # Used to judge the performance of the ANN\n",
    "                 ): \n",
    "        \"\"\"    \n",
    "        # TODO: specify defaults for required arguments\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.q_network = q_network\n",
    "        self.target_q_network = target_q_network\n",
    "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "        self.__compile(optimizer, loss_func,opt_metric)\n",
    "        self.memory = memory\n",
    "        #self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.num_burn_in = num_burn_in\n",
    "        self.batch_size = batch_size\n",
    "        self.trained_episodes = 0\n",
    "        self.max_ep_len = max_ep_length\n",
    "        self.output_dir = output_dir\n",
    "        self.experiment_id = experiment_id\n",
    "        self.summary_writer=summary_writer\n",
    "        self.itr = 0\n",
    "        \n",
    "        #self.learning_type = learning_type\n",
    "        \n",
    "    \n",
    "    def __compile(self, optimizer, loss_func, opt_metric):\n",
    "        \"\"\"Initialisation method, using the keras instance compile method. \"\"\"\n",
    "        \n",
    "        self.q_network.compile(optimizer, loss_func, opt_metric)\n",
    "        self.target_q_network.compile(optimizer, loss_func, opt_metric)\n",
    "        \n",
    "        \n",
    "    def fill_replay(self, env, policy):\n",
    "        \"\"\"Helper method for train. Fills the memory before model training begins.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env :  environment instance\n",
    "        policy : (str) policy to be used to fill memory\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Filling experience replay memory...\")\n",
    "        \n",
    "        env.start_simulation()\n",
    "        \n",
    "        for i in range(self.num_burn_in):\n",
    "            action = env.action.select_action(policy)\n",
    "            state, reward, nextstate, done = env.step(action)\n",
    "            self.memory.append(state, action, reward, nextstate)\n",
    "            \n",
    "        env.stop_simulation()\n",
    "        \n",
    "        print(\"...Done\")\n",
    "    \n",
    "    \n",
    "    def predict_q(self, network, state):\n",
    "        \"\"\"Helper method for train. Calls keras predict function using the keras model instance.\"\"\"\n",
    "        \n",
    "        return network.predict(state)\n",
    "    \n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Calls keras save function using the keras model instance\"\"\"\n",
    "        \n",
    "        filename =  \"{}/model_checkpoints/run{}_iter{}.h5\" .format(self.output_dir, \n",
    "                                               self.experiment_id, \n",
    "                                               self.itr)\n",
    "        self.q_network.save(filename)\n",
    "        \n",
    "        \n",
    "    def named_logs(self, q_network, logs):\n",
    "        \"\"\"create logs\"\"\"\n",
    "        \n",
    "        result = {}\n",
    "        for l in zip(q_network.metrics_names, logs):\n",
    "            result[l[0]] = l[1]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def update_network(self):\n",
    "        \"\"\"Helper method for train. Computes keras neural network updates using samples from memory.\n",
    "        \n",
    "        Notice that we want to incur in loss in the actions that we have selected.\n",
    "        Q_target and Q are set equal for not relevant actions so the loss is 0. \n",
    "        (weights not being updated due to these actions)\"\"\"\n",
    "        \n",
    "        # Sample mini batch\n",
    "        states_m, actions_m, rewards_m, states_m_p = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # attach q-values to states\n",
    "        target_batch = self.q_network.predict(states_m) \n",
    "        target_q = self.target_q_network.predict(states_m_p)\n",
    "        \n",
    "        # choose action\n",
    "        selected_actions = np.argmax(target_q, axis=1)\n",
    "        \n",
    "        # update q-values\n",
    "        for i, action in enumerate(selected_actions):\n",
    "            target_batch[i, action] =  rewards_m[i] + self.gamma * target_q[i, action]\n",
    "        \n",
    "        # keras method to train on batch that returns loss\n",
    "        loss = self.q_network.train_on_batch(states_m, target_batch)\n",
    "        \n",
    "        # get weights\n",
    "        weights = self.q_network.get_weights()\n",
    "        \n",
    "        # Update weights every target_update_freq steps\n",
    "        if self.itr % self.target_update_freq == 0:\n",
    "            self.target_q_network.set_weights(weights)\n",
    "        \n",
    "        # Storing Logs\n",
    "        if self.output_dir != None:\n",
    "            # Save network every save_after iterations\n",
    "            if self.itr % SAVE_AFTER == 0:\n",
    "                self.save()\n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def histo_summary(self, values, bins=1000):\n",
    "        \"\"\"Helper function in train method. Log a histogram of the tensor of values for tensorboard.\n",
    "        \n",
    "        Creates a HistogramProto instance that can be fed into Tensorboard.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        values :  (np.array) histogram values\n",
    "        bins : (int) how coarse the histogram is supposed to be\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins = bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "            \n",
    "        return hist\n",
    "         \n",
    "        \n",
    "    def train(self, env, num_episodes, policy, **kwargs):\n",
    "        \"\"\"Main method for the agent. Trains the keras neural network instances, calls all other helper methods.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env: (str) name of environment instance\n",
    "        num_episodes: (int) number of training episodes\n",
    "        policy: (str) name of policy to use to fill memory initially\n",
    "        \"\"\"\n",
    "        \n",
    "        all_stats = []\n",
    "        all_rewards = []\n",
    "        start_train_ep = self.trained_episodes\n",
    "        \n",
    "        for i in range(num_episodes):\n",
    "            # print progress of training\n",
    "            sys.stdout.write(\"\\r\"+'Running episode {} / {}'.format(self.trained_episodes+1, \n",
    "                                                       start_train_ep + num_episodes))\n",
    "            \n",
    "            env.start_simulation()\n",
    "            nextstate = env.obs.get()\n",
    "            done = False\n",
    "            \n",
    "            stats = {\n",
    "                'ep_id' : self.trained_episodes,\n",
    "                'total_reward': 0,\n",
    "                'episode_length': 0,\n",
    "                'max_q_value': 0, \n",
    "            }\n",
    "            \n",
    "            while not done and stats[\"episode_length\"] < self.max_ep_len:\n",
    "                \n",
    "                q_values = self.q_network.predict(nextstate)\n",
    "                action = env.action.select_action(policy, q_values = q_values, eps = eps)\n",
    "                state, reward, nextstate, done = env.step(action)\n",
    "                self.memory.append(state, action, reward, nextstate)\n",
    "                \n",
    "                # record loss for Tensorboard\n",
    "                loss = self.update_network()\n",
    "                \n",
    "                # create list of stats for Tensorboard, add scalars\n",
    "                summary_list = [tf.Summary.Value(tag = 'loss',\n",
    "                                                  simple_value = loss.item()),\n",
    "                                tf.Summary.Value(tag = 'Action 1',\n",
    "                                                  simple_value = self.q_network.layers[-1].get_weights()[1][0]),\n",
    "                                tf.Summary.Value(tag = 'Action 2',\n",
    "                                                  simple_value = self.q_network.layers[-1].get_weights()[1][1])]\n",
    "                \n",
    "                # add histogram of weights to list of stats for Tensorboard\n",
    "                for index, layer in enumerate(self.q_network.layers): \n",
    "                    \n",
    "                    if index != len(self.q_network.layers) - 1:\n",
    "                        summary_list.append(tf.Summary.Value(tag = str(layer.name) + \" weights\" ,\n",
    "                                                        histo = self.histo_summary(layer.get_weights()[0])))\n",
    "                        if len(layer.get_weights()) > 1:\n",
    "                            summary_list.append(tf.Summary.Value(tag = str(layer.name) + \" relu\" ,\n",
    "                                                        histo = self.histo_summary(layer.get_weights()[1])))\n",
    "                            \n",
    "                    else:\n",
    "                        summary_list.append(tf.Summary.Value(tag = str(layer.name) + \" output weights\" ,\n",
    "                                                        histo = self.histo_summary(layer.get_weights()[0])))\n",
    "                        summary_list.append(tf.Summary.Value(tag = \"output values\",\n",
    "                                                        histo = self.histo_summary(layer.get_weights()[1])))\n",
    "  \n",
    "                # write the list of stats to the logdd\n",
    "                self.summary_writer.add_summary(tf.Summary(value = summary_list), global_step=self.itr)\n",
    "                \n",
    "                self.itr += 1\n",
    "                \n",
    "                stats[\"ep_id\"] = self.trained_episodes\n",
    "                stats[\"episode_length\"] += 1\n",
    "                stats['total_reward'] += reward\n",
    "                stats['max_q_value'] += max(q_values)\n",
    "            \n",
    "            env.stop_simulation()\n",
    "            self.trained_episodes += 1\n",
    "        \n",
    "            all_stats.append(stats)\n",
    "            all_rewards.append(stats['total_reward'])\n",
    "            \n",
    "        print('\\nCurrent mean+std: {} {}'.format(np.mean(all_rewards),np.std(all_rewards)))\n",
    "\n",
    "        \n",
    "    def evaluate(self, env):\n",
    "        \"\"\"Use trained agent to run a simulation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env : environment instance\n",
    "        \"\"\"\n",
    "            \n",
    "        env.start_simulation()\n",
    "        nextstate = env.obs.get()\n",
    "        done = False\n",
    "        it = 0\n",
    "            \n",
    "        while not done and it < self.max_ep_len:\n",
    "                \n",
    "            q_values = self.q_network.predict(nextstate)\n",
    "            action = env.action.select_action(\"greedy\", q_values = q_values)\n",
    "            state, reward, nextstate,done = env.step(action)\n",
    "            it +=1\n",
    "                \n",
    "        env.stop_simulation()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/seb/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Filling experience replay memory...\n",
      " Retrying in 1 seconds\n",
      "...Done\n",
      "Running episode 1 / 20 Retrying in 1 seconds\n",
      "WARNING:tensorflow:From /Users/seb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Running episode 2 / 20 Retrying in 1 seconds\n",
      "Running episode 3 / 20 Retrying in 1 seconds\n",
      "Running episode 4 / 20 Retrying in 1 seconds\n",
      "Running episode 5 / 20 Retrying in 1 seconds\n",
      "Running episode 6 / 20 Retrying in 1 seconds\n",
      "Running episode 7 / 20 Retrying in 1 seconds\n",
      "Running episode 8 / 20 Retrying in 1 seconds\n",
      "Running episode 9 / 20 Retrying in 1 seconds\n",
      "Running episode 10 / 20 Retrying in 1 seconds\n",
      "Running episode 11 / 20 Retrying in 1 seconds\n",
      "Running episode 12 / 20 Retrying in 1 seconds\n",
      "Running episode 13 / 20 Retrying in 1 seconds\n",
      "Running episode 14 / 20 Retrying in 1 seconds\n",
      "Running episode 15 / 20 Retrying in 1 seconds\n",
      "Running episode 16 / 20 Retrying in 1 seconds\n",
      "Running episode 17 / 20 Retrying in 1 seconds\n",
      "Running episode 18 / 20 Retrying in 1 seconds\n",
      "Running episode 19 / 20 Retrying in 1 seconds\n",
      "Running episode 20 / 20 Retrying in 1 seconds\n",
      "\n",
      "Current mean+std: 8.295315600067031 27.328710850419107\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "##################################\n",
    "\n",
    "num_actions = 2\n",
    "state_shape = (1,9) # State var in rows\n",
    "memory_size = 100000\n",
    "gamma = 0.8\n",
    "target_update_frequency = 100\n",
    "num_init_samples_mem = 1000\n",
    "batch_size = 50\n",
    "max_episode_length = 100000\n",
    "optimizer = 'adam'\n",
    "loss = \"mse\"\n",
    "eps = 0.2\n",
    "env_name = \"Simple_Cross\"\n",
    "experiment_id = \"Test_Tensorboard\"\n",
    "monitoring = True # Store variables for TensorBoard monitoring and model_checkpoints\n",
    "\n",
    "\n",
    "\n",
    "# Define logs directory if monitoring enabled\n",
    "    output_dir = get_output_folder(\"./Logs\",experiment_id)\n",
    "    summary_writer = tf.summary.FileWriter(logdir=output_dir)\n",
    "else:\n",
    "    output_dir = None\n",
    "    summary_writer = None\n",
    "\n",
    "\n",
    "# Initialize Q-networks (value and target)\n",
    "q_network = get_model('simple',(state_shape[1],),num_actions)\n",
    "target_q_network = get_model('simple',(state_shape[1],),num_actions)\n",
    "\n",
    "# Initialize environment\n",
    "sumo_env =  Env(    \"cross.net.xml\",\n",
    "                    \"cross.rou.xml\",\n",
    "                    state_shape,\n",
    "                    num_actions,\n",
    "                    use_gui=False\n",
    "               )\n",
    "\n",
    "# Initialize replay memory\n",
    "memory = ReplayMemory(    memory_size,\n",
    "                          state_shape,\n",
    "                          num_actions\n",
    "                     )\n",
    "\n",
    "# Initialize Double DQN algorithm\n",
    "ddqn = DoubleDQN(   q_network,\n",
    "                    target_q_network,\n",
    "                    memory,\n",
    "                    gamma,\n",
    "                    target_update_frequency,\n",
    "                    num_init_samples_mem,\n",
    "                    batch_size,\n",
    "                    optimizer,\n",
    "                    loss,\n",
    "                    max_episode_length,\n",
    "                    sumo_env,\n",
    "                    output_dir,\n",
    "                    experiment_id,\n",
    "                    summary_writer\n",
    "                )\n",
    "\n",
    "# Fill Replay Memory\n",
    "ddqn.fill_replay(sumo_env,'rand')\n",
    "\n",
    "# Trains Double DQN\n",
    "ddqn.train(  sumo_env, 20, \"epsGreedy\", eps=eps)\n",
    "\n",
    "#### How to start a Tensorboard\n",
    "# run the below command in terminal to start Tensorboard, then open http://localhost:6006/ in a browser\n",
    "# !tensorboard --logdir=./Scripts/Tests_Sumo/cross_RL/Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluates Double DQN\n",
    "sumo_env =  Env(    \"cross.net.xml\",\n",
    "                    \"cross.rou.xml\",\n",
    "                    state_shape,\n",
    "                    num_actions,\n",
    "                    use_gui=True\n",
    "               )\n",
    "ddqn.evaluate( sumo_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "801.85px",
    "left": "1673px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
