{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MEMORY CLASS\n",
    "##################################################\n",
    "\"\"\" \n",
    "Store all transitions and methods to manipulate, get, assign them\n",
    "\"\"\"\n",
    "\n",
    "class SingleSample():\n",
    "    def __init__(self, state_shape, num_actions): #Num actions not used up to now\n",
    "        self.state = np.zeros(state_shape)\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        self.nextstate = np.zeros(state_shape)\n",
    "        \n",
    "    def assign(self, state, action, reward, nextstate):\n",
    "        self.state[:] = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.nextstate[:] = nextstate\n",
    "        \n",
    "    def print_obs(self):\n",
    "        print( \"State: \\n\\n\",self.state,\n",
    "               \"\\n\\nAction:\\n\\n\",self.action,\n",
    "               \"\\n\\nReward:\\n\\n\",self.reward,\n",
    "               \"\\n\\nNext State:\\n\\n\",self.nextstate)\n",
    "    \n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, max_size, state_shape, num_actions):\n",
    "    # initialize the whole memory at once\n",
    "        self.memory = [SingleSample(state_shape,num_actions) for _ in range(max_size)]\n",
    "        self.max_size = max_size\n",
    "        self.itr = 0  # insert the next element here\n",
    "        self.cur_size = 0\n",
    "    \n",
    "    def append(self, state, action, reward, nextstate):\n",
    "        self.memory[self.itr].assign(state, action, reward, nextstate)\n",
    "        self.itr += 1\n",
    "        self.cur_size = min(self.cur_size + 1, self.max_size)\n",
    "        self.itr %= self.max_size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        # Uniform sampling, later prioritized experience replay can be implemented\n",
    "        states, actions,rewards,next_states = [],[],[],[]\n",
    "        for i, idx in enumerate(np.random.randint(0, self.cur_size, size=batch_size)):\n",
    "            transition = self.memory[idx]\n",
    "            states.append(transition.state)\n",
    "            actions.append(transition.action)\n",
    "            rewards.append(transition.reward)\n",
    "            next_states.append(transition.nextstate)\n",
    "        return np.vstack(states), actions, rewards, np.vstack(next_states)\n",
    "    \n",
    "    def print_obs(self,obs):\n",
    "        self.memory[obs].print_obs()\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.cur_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [
     0,
     24
    ]
   },
   "outputs": [],
   "source": [
    "# ROUTING (DEMAND)\n",
    "#################################################\n",
    "\n",
    "### TO DO\n",
    "# Create a class to spcify different types of demand\n",
    "\n",
    "def generate_routefile():\n",
    "    random.seed(42)  # make tests reproducible\n",
    "    N = 3600  # number of time steps\n",
    "    # demand per second from different directions\n",
    "\n",
    "    pEW = 1 / 10\n",
    "    pNS = 1 / 40\n",
    "    pWE = 1 / 10\n",
    "    pSN = 1 / 40\n",
    "\n",
    "    with open(\"cross.rou.xml\", \"w\") as routes:\n",
    "        print(\"\"\"<routes>\n",
    "        <vType id=\"car\" accel=\"0.8\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"16.67\" guiShape=\"passenger\"/>\n",
    "        <route id=\"right\" edges=\"51o 1i 2o 52i\" />\n",
    "        <route id=\"left\" edges=\"52o 2i 1o 51i\" />\n",
    "        <route id=\"down\" edges=\"54o 4i 3o 53i\" />\n",
    "        <route id=\"up\" edges=\"53o 3i 4o 54i\" />\"\"\", file=routes)\n",
    "        vehNr = 0\n",
    "        for i in range(N):\n",
    "            if random.uniform(0, 1) < pWE:\n",
    "                print('    <vehicle id=\"right_%i\" type=\"car\" route=\"right\" depart=\"%i\" />' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pEW:\n",
    "                print('    <vehicle id=\"left_%i\" type=\"car\" route=\"left\" depart=\"%i\" />' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pNS:\n",
    "                print('    <vehicle id=\"down_%i\" type=\"car\" route=\"up\" depart=\"%i\" color=\"1,0,0\"/>' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pSN:\n",
    "                print('    <vehicle id=\"UP_%i\" type=\"car\" route=\"down\" depart=\"%i\" color=\"1,0,0\"/>' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "        print(\"</routes>\", file=routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Q NETWORKS\n",
    "################################\n",
    "\n",
    "# More ANN arquitectures to be specified here\n",
    "\n",
    "def get_model(model_name, *args):\n",
    "    if model_name == 'linear':\n",
    "        return linear(*args)\n",
    "    elif model_name == 'simple':\n",
    "        return simple(*args)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "def linear(input_shape, num_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape)) # If a vector this does not have any effect. (only matrices)\n",
    "    model.add(Dense(num_actions,activation=None))\n",
    "    return model\n",
    "\n",
    "def simple(input_shape, num_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(18, input_shape = input_shape,activation='relu'))\n",
    "    model.add(Dense(18, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation=None))\n",
    "    return model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ENVIRONMENT CLASS\n",
    "##################################\n",
    "import os, sys\n",
    "\n",
    "# Making sure path to SUMO bins correctly specified\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "import sumolib\n",
    "import traci\n",
    "    \n",
    "class Env:\n",
    "    \n",
    "    \"\"\"\n",
    "    SUMO Environment for Traffic Signal Control\n",
    "    net_file: (str) SUMO .net.xml file\n",
    "    route_file: (str) SUMO .rou.xml file\n",
    "    use_gui: (bool) Wheter to run SUMO simulation with GUI visualisation\n",
    "    delta_time: (int) Simulation seconds between actions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 net_file, \n",
    "                 route_file,\n",
    "                 state_shape,\n",
    "                 num_actions,\n",
    "                 use_gui = False,\n",
    "                 delta_time=10):\n",
    "        \n",
    "        self.net = net_file\n",
    "        self.route = route_file\n",
    "        self.use_gui = use_gui\n",
    "        self.time_step = delta_time\n",
    "        if self.use_gui:\n",
    "            self.sumo_binary = sumolib.checkBinary('sumo-gui')\n",
    "        else:\n",
    "            self.sumo_binary = sumolib.checkBinary('sumo')\n",
    "            \n",
    "        self.obs = Observation(state_shape)\n",
    "        self.action = Action(num_actions)\n",
    " \n",
    "    def start_simulation(self):\n",
    "        \n",
    "        sumo_cmd = [self.sumo_binary, \n",
    "                    '-n', self.net,\n",
    "                    '-r' ,self.route]\n",
    "        \n",
    "        if self.use_gui:\n",
    "            sumo_cmd.append('--start')\n",
    "        time.sleep(10)\n",
    "        traci.start(sumo_cmd)\n",
    "        time.sleep(10)\n",
    "        self.obs.update_state()\n",
    "        \n",
    "    def take_action(self,action):\n",
    "        #action = 0 -> row vertically\n",
    "        if action == 0 and traci.trafficlight.getPhase(\"0\") != 0:\n",
    "            traci.trafficlight.setPhase(\"0\",3)\n",
    "        #action = 1 -> row horizontally    \n",
    "        elif action == 1 and traci.trafficlight.getPhase(\"0\") != 2:\n",
    "            traci.trafficlight.setPhase(\"0\",1)\n",
    "                \n",
    "    def compute_reward(self,state,next_state):\n",
    "        # Here is whre reward is specified\n",
    "        a = next_state - state\n",
    "        b = np.round(next_state,decimals=1)\n",
    "\n",
    "        aux = np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "        \n",
    "        return -np.sum(next_state[0,:4])#np.sum(a[0,4:-1])-np.sum(a[0,:4]) # - delta number of stopped cars\n",
    "        \n",
    "    def step( self, action):\n",
    "        \n",
    "        state = copy.deepcopy(self.obs.get())\n",
    "        \n",
    "        if action != None:\n",
    "            self.take_action(action)\n",
    "        \n",
    "        traci.simulationStep(traci.simulation.getTime() + self.time_step) # Run the simulation time_step (s)\n",
    "        \n",
    "        self.obs.update_state()\n",
    "        next_state = self.obs.get()\n",
    "        \n",
    "        reward = self.compute_reward(state,next_state)\n",
    "        \n",
    "        return state, reward, next_state, self.done()\n",
    "    \n",
    "    def done(self):\n",
    "        return traci.simulation.getMinExpectedNumber() == 0\n",
    "    \n",
    "    def stop_simulation(self):\n",
    "        traci.close()\n",
    "        \n",
    "\n",
    "    \n",
    "class Observation:\n",
    "    def __init__(self,\n",
    "                 shape):\n",
    "        self.obs = np.zeros(shape)\n",
    "        \n",
    "    def update_state(self):\n",
    "        \n",
    "        lanes = [\"4i_0\",\"2i_0\",\"3i_0\",\"1i_0\"]           \n",
    "        for i,lane in enumerate(lanes):\n",
    "\n",
    "            self.obs[:,i] = traci.lane.getLastStepHaltingNumber(lane)\n",
    "            self.obs[:,i+4] = traci.lane.getLastStepMeanSpeed(lane)\n",
    "        \n",
    "        self.obs[:,8] = traci.trafficlight.getPhase(\"0\")\n",
    "        \n",
    "    def get(self):\n",
    "        return self.obs\n",
    "        \n",
    "    def get_reward(self):\n",
    "        return -np.sum(self.obs[0:4])\n",
    "    \n",
    "class Action:\n",
    "    \"\"\" One-hot encoding of the phase of the traffic signal\"\"\"\n",
    "    def __init__( self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.action_space = np.identity(num_actions)\n",
    "        \n",
    "    def select_action(self,policy, **kwargs):\n",
    "        if policy == \"randUni\":\n",
    "            return self.select_randuni()\n",
    "        elif policy == \"greedy\":\n",
    "            # NOt implemented yet\n",
    "            return self.select_greedy(**kwargs)\n",
    "        elif policy == \"epsGreedy\":\n",
    "            # Not implemented yet\n",
    "            return self.select_epsgreedy(**kwargs)\n",
    "    \n",
    "    def select_randuni(self):\n",
    "        return np.random.randint(0, self.num_actions)\n",
    "    \n",
    "    def select_greedy(self, q_values):\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def select_epsgreedy(self,eps,q_values):\n",
    "        \n",
    "        val = np.random.uniform()\n",
    "        if val < eps:\n",
    "            return self.select_randuni()\n",
    "        else:\n",
    "            return self.select_greedy(q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# DOUBLE DQN\n",
    "################################\n",
    "\n",
    "class DoubleDQN:\n",
    "    def __init__(self,\n",
    "                 q_network, \n",
    "                 target_q_network,\n",
    "                 memory, # Replay memory\n",
    "                 #policy,\n",
    "                 gamma, # Discount factor\n",
    "                 target_update_freq, # Frequency to update target network\n",
    "                 num_burn_in, # How many samples to fill replay memory\n",
    "                 #train_freq,\n",
    "                 batch_size,\n",
    "                 optimizer,\n",
    "                 loss_func,\n",
    "                 max_ep_length,\n",
    "                 opt_metric=None\n",
    "                 ): # Used to judge the performance of the ANN\n",
    "        self.q_network = q_network\n",
    "        self.target_q_network = target_q_network\n",
    "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "        self.__compile(optimizer, loss_func,opt_metric)\n",
    "        self.memory = memory\n",
    "        #self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.num_burn_in = num_burn_in\n",
    "        #self.train_freq = train_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.training_reward_seen = 0\n",
    "        self.trained_episodes = 0\n",
    "        self.max_ep_len = max_ep_length\n",
    "        #self.learning_type = learning_type\n",
    "        \n",
    "    \n",
    "    def __compile(self, optimizer, loss_func, opt_metric):\n",
    "        \n",
    "        self.q_network.compile(optimizer, loss_func, opt_metric)\n",
    "        self.target_q_network.compile(optimizer, loss_func, opt_metric)\n",
    "        \n",
    "    def fill_replay(self,env,policy):\n",
    "        \n",
    "        print(\"Filling experience replay memory...\")\n",
    "        \n",
    "        sumo_env.start_simulation()\n",
    "        \n",
    "        for i in range(self.num_burn_in):\n",
    "            action = env.action.select_action(policy)\n",
    "            state, reward, nextstate,done = env.step(action)\n",
    "            self.memory.append(state,action,reward,nextstate)\n",
    "            \n",
    "        sumo_env.stop_simulation()\n",
    "        \n",
    "        print(\"Done\")\n",
    "    \n",
    "    def predict_q(self,network,state):\n",
    "        return network.predict(state)\n",
    "    \n",
    "    def fit(self,itr):\n",
    "        \n",
    "        # Sample mini batch\n",
    "        states_m,actions_m,rewards_m,states_m_p = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Compute target\n",
    "        # Notice that we want to incur in loss in the actions that we have selected.\n",
    "        # Q_target and Q are set equal for not relevant actions so the loss is 0. \n",
    "        # (weights not being updated due to these actions)\n",
    "        target_batch = self.q_network.predict(states_m) \n",
    "        \n",
    "        target_q = self.target_q_network.predict(states_m_p)\n",
    "        selected_actions = np.argmax(target_q, axis=1 )\n",
    "        \n",
    "        for i,action in enumerate(selected_actions):\n",
    "            target_batch[i,action] =  rewards_m[i] + self.gamma * np.max(target_q[i])\n",
    "\n",
    "        self.q_network.train_on_batch(states_m,target_batch)\n",
    "        \n",
    "        # Update weights every target_update_freq steps\n",
    "        if itr % self.target_update_freq == 0:\n",
    "            self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "            \n",
    "    def run_episodes(self,env,num_episodes,policy,**kwargs):\n",
    "        \n",
    "        all_stats = []\n",
    "        all_rewards = []\n",
    "        \n",
    "        start_train_ep = self.trained_episodes\n",
    "        \n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            print('Running episode {} / {}'.format(self.trained_episodes+1, start_train_ep + num_episodes))\n",
    "            \n",
    "            env.start_simulation()\n",
    "            nextstate = env.obs.get()\n",
    "            done = False\n",
    "            \n",
    "            stats = {\n",
    "                'ep_id' : self.trained_episodes,\n",
    "                'total_reward': 0,\n",
    "                'episode_length': 0,\n",
    "                'max_q_value': 0,\n",
    "                \n",
    "            }\n",
    "            \n",
    "            while not done and stats[\"episode_length\"] < self.max_ep_len:\n",
    "                \n",
    "                q_values = self.q_network.predict(nextstate)\n",
    "                action = env.action.select_action(policy, q_values = q_values, eps = eps)\n",
    "                state, reward, nextstate,done = env.step(action)\n",
    "                self.memory.append(state,action,reward,nextstate)\n",
    "                \n",
    "                self.fit(stats[\"episode_length\"])                      \n",
    "                \n",
    "                stats[\"ep_id\"] = self.trained_episodes\n",
    "                stats[\"episode_length\"] += 1\n",
    "                stats['total_reward'] += reward\n",
    "                stats['max_q_value'] += max(q_values)\n",
    "            \n",
    "            env.stop_simulation()\n",
    "            self.trained_episodes += 1\n",
    "        \n",
    "            all_stats.append(stats)\n",
    "            all_rewards.append(stats['total_reward'])\n",
    "        print('Current mean+std: {} {}'.format(np.mean(all_rewards),np.std(all_rewards)))\n",
    "        \n",
    "        return(all_stats, all_rewards)\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling experience replay memory...\n",
      " Retrying in 1 seconds\n",
      "Done\n",
      "Running episode 1 / 10\n",
      " Retrying in 1 seconds\n",
      "Running episode 2 / 10\n",
      " Retrying in 1 seconds\n",
      "Running episode 3 / 10\n",
      " Retrying in 1 seconds\n",
      "Running episode 4 / 10\n",
      " Retrying in 1 seconds\n",
      "Running episode 5 / 10\n",
      " Retrying in 1 seconds\n",
      "Running episode 6 / 10\n",
      " Retrying in 1 seconds\n",
      "Running episode 7 / 10\n",
      " Retrying in 1 seconds\n",
      "Running episode 8 / 10\n",
      " Retrying in 1 seconds\n",
      "Running episode 9 / 10\n",
      " Retrying in 1 seconds\n",
      "Running episode 10 / 10\n",
      " Retrying in 1 seconds\n",
      "Current mean+std: -25426.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'ep_id': 9,\n",
       "   'total_reward': -25426.0,\n",
       "   'episode_length': 527,\n",
       "   'max_q_value': array([-118592.54 , -124206.195], dtype=float32)}],\n",
       " [-25426.0])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAIN\n",
    "##################################\n",
    "\n",
    "num_actions = 2\n",
    "state_shape = (1,9) # State var in rows\n",
    "memory_size = 100000\n",
    "gamma = 0.8\n",
    "target_update_frequency = 100\n",
    "num_init_samples_mem = 1000\n",
    "batch_size = 50\n",
    "max_episode_length = 100000\n",
    "optimizer = 'adam'\n",
    "loss = \"mse\"\n",
    "eps = 0.2\n",
    "\n",
    "# Initialize Q-networks (value and target)\n",
    "q_network = get_model('simple',(state_shape[1],),num_actions)\n",
    "target_q_network = get_model('simple',(state_shape[1],),num_actions)\n",
    "\n",
    "# Initialize environment\n",
    "sumo_env =  Env(    \"cross.net.xml\",\n",
    "                    \"cross.rou.xml\",\n",
    "                    state_shape,\n",
    "                    num_actions,\n",
    "                    use_gui=False\n",
    "               )\n",
    "\n",
    "# Initialize replay memory\n",
    "memory = ReplayMemory(    memory_size,\n",
    "                          state_shape,\n",
    "                          num_actions\n",
    "                     )\n",
    "\n",
    "# Initialize Double DQN algorithm\n",
    "ddqn = DoubleDQN(   q_network,\n",
    "                    target_q_network,\n",
    "                    memory,\n",
    "                    gamma,\n",
    "                    target_update_frequency,\n",
    "                    num_init_samples_mem,\n",
    "                    batch_size,\n",
    "                    optimizer,\n",
    "                    loss,\n",
    "                    max_episode_length\n",
    "                )\n",
    "\n",
    "# Fill Replay Memory\n",
    "ddqn.fill_replay(sumo_env,'randUni')\n",
    "\n",
    "# Run episodes\n",
    "ddqn.run_episodes(   sumo_env,\n",
    "                     10,\n",
    "                     \"epsGreedy\",\n",
    "                     eps=eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Not consider this part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    }
   ],
   "source": [
    "sumo_env =  Env(    \"cross_no_RL.net.xml\",\n",
    "                    \"cross.rou.xml\",\n",
    "                    state_shape,\n",
    "                    num_actions,\n",
    "                    use_gui=True\n",
    "               )\n",
    "sumo_env.start_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "memory = ReplayMemory(    memory_size,\n",
    "                          state_shape,\n",
    "                          num_actions\n",
    "                     )\n",
    "while not done:\n",
    "    a,b,c,done = sumo_env.step(None)\n",
    "    memory.append(a,1,b,c)\n",
    "    \n",
    "sumo_env.stop_simulation()\n",
    "\n",
    "sumo_env.start_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     18
    ]
   },
   "outputs": [],
   "source": [
    "def run_DDQN():\n",
    "    \"\"\"execute the TraCI control loop\"\"\"\n",
    "    # Initialize Q-networks with random values\n",
    "    model_target = define_ANN()\n",
    "    model_value = define_ANN()\n",
    "\n",
    "    # Initialize params\n",
    "    y = 0.9  # how much to value future rewards (long-term max vs short-term max)\n",
    "    eps = 0.2 # how much exploration\n",
    "    decay_factor = 0.999 # how much to lower exploration as learning goes on\n",
    "    reward_hist = [] # average reward collected\n",
    "    num_episodes = 1 # Number of episodes to run\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "\n",
    "        # Initialise SUMO\n",
    "        sumoBinary = checkBinary('sumo')\n",
    "        traci.start([sumoBinary, \"-c\", \"cross.sumocfg\",\"--tripinfo-output\", \"tripinfo.xml\"])\n",
    "\n",
    "        # Number of observations in replay database\n",
    "        n = 10\n",
    "\n",
    "        # Initialize replay database with random actions\n",
    "        states,actions,rewards = init_replay(n)\n",
    "\n",
    "        # Printing episode info\n",
    "        print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
    "\n",
    "        reward_sum = 0                                                             # initialise reward sum\n",
    "        step = 0\n",
    "        M = 200\n",
    "        eps *= decay_factor\n",
    "        \n",
    "        # Run simulation until there is no more cars in the network\n",
    "        while traci.simulation.getMinExpectedNumber() > 0:\n",
    "\n",
    "            # Run 10 more sec of simulations\n",
    "            traci.simulationStep(10*(n+2))\n",
    "            # Collecting reward of action\n",
    "            rewards.append(get_reward(states[-1]))\n",
    "            # Observe new state\n",
    "            states = np.vstack((states,get_state()))\n",
    "            \n",
    "            \n",
    "\n",
    "            # take action randomly eps [%] times\n",
    "            if np.random.random() < eps:\n",
    "                # Choose action randomly and apply it to the network\n",
    "                actions.append(np.random.choice([0,1]))\n",
    "                take_action(actions[-1])\n",
    "            else:\n",
    "                # choose action that maximises predicted reward on value model\n",
    "                actions.append(np.argmax(model_value.predict(states[-1].reshape(1,9))))\n",
    "                take_action(actions[-1])\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Perform model updates\n",
    "            # Sample mini-batch (m) of transitions from D,actions,reward_sum\n",
    "            states_m,actions_m,rewards_m,states_m_p = sample_transitions(states,actions,rewards,M)\n",
    "            \n",
    "            # Value update\n",
    "            import pdb; pdb.set_trace()\n",
    "            \n",
    "            target = rewards_m + y * np.max(model_target.predict(states_m_p),axis =0)\n",
    "            value = model_value.predict(states_m)\n",
    "            \n",
    "            \n",
    "            \n",
    "            model_value.fit(states_m, target, epochs=1, verbose=0)\n",
    "            \n",
    "            if step % M == 0:\n",
    "                model_target.set_weights(model_value.get_weights())          \n",
    "            \n",
    "                \n",
    "            step += 1\n",
    "        reward_hist.append(np.sum(rewards))\n",
    "        traci.close()\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return reward_hist\n",
    "\n",
    "def run_wo():\n",
    "    \"\"\"execute the TraCI control loop\"\"\"\n",
    "\n",
    "\n",
    "    reward_avg_list = []                                                            # average reward collected\n",
    "    num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        #state = env.reset()                                                         # when called, resets the environment\n",
    "        #eps *= decay_factor                                                         # reduces the exploration rate\n",
    "        print(\"Episode {} of {}\".format(i + 1, num_episodes))                       # print progress\n",
    "        reward_sum = 0                                                            # initialise reward sum\n",
    "        step = 0\n",
    "        traci.simulationStep(10)                                                    # Initialise simulation\n",
    "        while traci.simulation.getMinExpectedNumber() > 0:                          # Run simulation until there is no more cars in the network\n",
    "\n",
    "            print(step)\n",
    "            # Get current state\n",
    "\n",
    "            traci.simulationStep(10*step)                                                # Simulation step 10 s\n",
    "\n",
    "            new_state = get_state()\n",
    "            reward = get_reward(new_state)\n",
    "\n",
    "            reward_sum += reward\n",
    "            reward_hist.append(reward_sum)\n",
    "            step += 1\n",
    "        #r_avg_list.append(reward_sum / 100)\n",
    "\n",
    "        return reward_hist\n",
    "\n",
    "\n",
    "    traci.close()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumoBinary = checkBinary('sumo')\n",
    "\n",
    "# first, generate the route file for this simulation\n",
    "generate_routefile()\n",
    "\n",
    "# this is the normal way of using traci. sumo is started as a\n",
    "# subprocess and then the python script connects and runs\n",
    "traci.start([sumoBinary, \"-c\", \"cross.sumocfg\"])\n",
    "reward_hist_NN = run_DDQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# this is the main entry point of this script\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sumoBinary = checkBinary('sumo')\n",
    "\n",
    "    # first, generate the route file for this simulation\n",
    "    generate_routefile()\n",
    "\n",
    "    # this is the normal way of using traci. sumo is started as a\n",
    "    # subprocess and then the python script connects and runs\n",
    "    traci.start([sumoBinary, \"-c\", \"cross.sumocfg\"])\n",
    "    reward_hist_NN = run_DDQN()\n",
    "\n",
    "    traci.start([sumoBinary, \"-c\", \"cross_no_RL.sumocfg\"])\n",
    "    reward_hist = run_wo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "    plt.plot(reward_hist)\n",
    "    plt.plot(reward_hist_NN)\n",
    "    plt.legend([\"Rw_hist wo RL\",\"Rw_hist w RL\"])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
