{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "##########################\n",
    "\n",
    "import random\n",
    "import copy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "import os, sys\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Making sure path to SUMO bins correctly specified\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "import sumolib\n",
    "import traci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# OUTPUT FILE FOLDER\n",
    "####################################\n",
    "\n",
    "def get_output_folder(parent_dir, exp_id):\n",
    "    \"\"\"Return save folder parent_dir/Results/exp_id\n",
    "    \n",
    "    If this directory already exists it creates parent_dir/Results/exp_id_{i}, \n",
    "    being i the next smallest free number.\n",
    "    \n",
    "    Inside this directory it also creates a sub-directory called model_checkpoints to \n",
    "    store intermediate training steps.\n",
    "    \n",
    "    This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir : str\n",
    "        Path of the directory where results will be stored.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/Results/exp_id\n",
    "        Path to this run's save directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Returns an error if parent_dir already exists\n",
    "        os.makedirs(parent_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if exp_id in os.listdir(parent_dir):\n",
    "        \n",
    "        experiment_id = 1\n",
    "        new_folder = os.path.join(parent_dir,exp_id+\"_\"+str(experiment_id))\n",
    "        \n",
    "        while os.path.exists(new_folder):\n",
    "            experiment_id +=1\n",
    "            new_folder = os.path.join(parent_dir,exp_id+\"_\"+str(experiment_id))\n",
    "        \n",
    "        parent_dir = new_folder\n",
    "        os.makedirs(parent_dir)\n",
    "        os.mkdir(os.path.join(parent_dir,\"model_checkpoints\"))\n",
    "    else:\n",
    "        parent_dir = os.path.join(parent_dir,exp_id)\n",
    "        os.makedirs(parent_dir)\n",
    "        os.mkdir(os.path.join(parent_dir,\"model_checkpoints\"))\n",
    "        \n",
    "    return parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MEMORY CLASS\n",
    "##################################################\n",
    "\n",
    "class ReplayMemory():\n",
    "    \"\"\" \n",
    "    Keeps a memory of transitions the environment has revealed as response to actions taken.\n",
    "    \n",
    "    Keeps a memory of size max_size, implemented through lists of class SingleSample: state, action, reward and nextstate.\n",
    "    An index counter is pushed forward, through the list as the memory is filled. Once memory is full\n",
    "    the index starts from 0, and overwrites existing memory. The index counter is also used to indicate\n",
    "    which one the most recent memory is, to allow prioritising more recent memory over older memory.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    memory: (list) list of class SingleSample containing memory of max_size of transitions\n",
    "    max_size : (int) memory capacity required\n",
    "    itr : (int) current index\n",
    "    cur_size : (int) current size of memory\n",
    "    \n",
    "    Methods\n",
    "    ------\n",
    "    append(state, action, reward, nextstate)\n",
    "        adds new elements to the four lists, handles index counter\n",
    "        \n",
    "    sample(batch_size)\n",
    "        Randomly draws a sample of batch_size of transitions. It returns 2 arrays and two lists.\n",
    "        The arrays correspond to state and next state of the transitions with dimensions \n",
    "        (batch_size, space_shape).The lists correspond to actions and rewards of those transitions.\n",
    "        \n",
    "    print_obs(obs)\n",
    "        prints a specific transition\n",
    "        \n",
    "    get_size\n",
    "        returns the current size of the memory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size, state_shape, num_actions):\n",
    "        \"\"\"Initialize the whole memory at once.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        max_size : (int) memory capacity required\n",
    "        state_shape : (tuple) tuple specifying the shape of the array in which state variables are stored.\n",
    "        num_actions : (int) number of actions (traffic signal phases)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory = [SingleSample(state_shape,num_actions) for _ in range(max_size)]\n",
    "        self.max_size = max_size\n",
    "        self.itr = 0  # insert the next element here\n",
    "        self.cur_size = 0\n",
    "    \n",
    "    \n",
    "    def append(self, state, action, reward, nextstate):\n",
    "        \"\"\"Adds new elements to the four lists, handles index counter.\n",
    "                \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : (np.array) all state variables for one observation\n",
    "        action : (int) index of the one-hot encoded vector indicating action\n",
    "        reward : (float) reward after action taken\n",
    "        nextstate : (np.array) all state variables for one observation, after action taken\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory[self.itr].assign(state, action, reward, nextstate)\n",
    "        self.itr += 1\n",
    "        self.cur_size = min(self.cur_size + 1, self.max_size)\n",
    "        self.itr %= self.max_size\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Uniform sampling, later prioritized experience replay can be implemented.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : (int) size of the batch to be sampled\n",
    "        \"\"\"\n",
    "\n",
    "        states, actions, rewards, next_states = [],[],[],[]\n",
    "        for i, idx in enumerate(np.random.randint(0, self.cur_size, size=batch_size)):\n",
    "            transition = self.memory[idx]\n",
    "            states.append(transition.state)\n",
    "            actions.append(transition.action)\n",
    "            rewards.append(transition.reward)\n",
    "            next_states.append(transition.nextstate)\n",
    "        return np.vstack(states), actions, rewards, np.vstack(next_states)\n",
    "    \n",
    "    \n",
    "    def print_obs(self,obs):\n",
    "        \"\"\"Selects a specific transition to view.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : (int) index of the specific transition to view\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory[obs].print_obs() # This calls a SingleSample method called also print_obs\n",
    "\n",
    "        \n",
    "    def get_size(self):\n",
    "        \"\"\"Shows current size of memory\"\"\"\n",
    "        \n",
    "        return self.cur_size\n",
    "    \n",
    "    \n",
    "    \n",
    "class SingleSample():\n",
    "    \"\"\"A helper for the memory class. It stores single transition objects.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    state : (np.array) all state variables for one observation\n",
    "    action : (int) index of the one-hot encoded vector indicating action\n",
    "    reward : (int) reward after action taken\n",
    "    nextstate : (np.array) all state variables for one observation, after action taken\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    assign(self, state, action, reward, nextstate)\n",
    "        Assigns new values to attributes.\n",
    "        \n",
    "    print_obs()\n",
    "        Prints current observation/ transition.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_shape, num_actions): # Num actions not used up to now\n",
    "        \"\"\"Initialises object instance.\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_shape : (tuple) (tuple) tuple specifying the shape of the array in which state variables are stored.\n",
    "        num_actions : (int) number of actions (traffic signal phases)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state = np.zeros(state_shape)\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        self.nextstate = np.zeros(state_shape)\n",
    "        \n",
    "        \n",
    "    def assign(self, state, action, reward, nextstate):\n",
    "        \"\"\"Assigns new values to attributes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : (np.array) all state variables for one observation\n",
    "        action : (np.array) index of the one-hot encoded vector indicating action\n",
    "        reward : (int) reward after action taken\n",
    "        nextstate : (np.array) all state variables for one observation, after action taken\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state[:] = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.nextstate[:] = nextstate\n",
    "        \n",
    "        \n",
    "    def print_obs(self):\n",
    "        \"\"\"Prints current observation\"\"\"\n",
    "        \n",
    "        print( \"State: \\n\\n\",self.state,\n",
    "               \"\\n\\nAction:\\n\\n\",self.action,\n",
    "               \"\\n\\nReward:\\n\\n\",self.reward,\n",
    "               \"\\n\\nNext State:\\n\\n\",self.nextstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ROUTING (DEMAND)\n",
    "#################################################\n",
    "\n",
    "### TO DO\n",
    "# Create a class to specify different types of demand\n",
    "\n",
    "def generate_routefile():\n",
    "    \"\"\"Returns XML file specifying network layout for sumo simulation\"\"\"\n",
    "    random.seed(42)  # make tests reproducible\n",
    "    N = 3600  # number of time steps\n",
    "    # demand per second from different directions\n",
    "\n",
    "    pEW = 1 / 20\n",
    "    pNS = 1 / 80\n",
    "    pWE = 1 / 20\n",
    "    pSN = 1 / 80\n",
    "\n",
    "    with open(\"cross.rou.xml\", \"w\") as routes:\n",
    "        print(\"\"\"<routes>\n",
    "        <vType id=\"car\" accel=\"0.8\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"16.67\" guiShape=\"passenger\"/>\n",
    "        <route id=\"right\" edges=\"51o 1i 2o 52i\" />\n",
    "        <route id=\"left\" edges=\"52o 2i 1o 51i\" />\n",
    "        <route id=\"down\" edges=\"54o 4i 3o 53i\" />\n",
    "        <route id=\"up\" edges=\"53o 3i 4o 54i\" />\"\"\", file=routes)\n",
    "        vehNr = 0\n",
    "        for i in range(N):\n",
    "            if random.uniform(0, 1) < pWE:\n",
    "                print('    <vehicle id=\"right_%i\" type=\"car\" route=\"right\" depart=\"%i\" />' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pEW:\n",
    "                print('    <vehicle id=\"left_%i\" type=\"car\" route=\"left\" depart=\"%i\" />' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pNS:\n",
    "                print('    <vehicle id=\"down_%i\" type=\"car\" route=\"up\" depart=\"%i\" color=\"1,0,0\"/>' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pSN:\n",
    "                print('    <vehicle id=\"UP_%i\" type=\"car\" route=\"down\" depart=\"%i\" color=\"1,0,0\"/>' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "        print(\"</routes>\", file=routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Q NETWORKS\n",
    "################################\n",
    "\n",
    "# More ANN arquitectures to be specified here\n",
    "def get_model(model_name, *args):\n",
    "    \"\"\"Helper function to instantiate q-networks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : (str) Name of the network architecture to be used to instantiate q-network\n",
    "    *args : arguments to be passed onto helper functions\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name == 'linear':\n",
    "        return linear(*args)\n",
    "    elif model_name == 'simple':\n",
    "        return simple(*args)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "        \n",
    "def linear(input_shape, num_actions):\n",
    "    \"\"\"Feeds into get_model. Sets up a linear keras model instance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : (np.array) shape of the state vector to be fed into the model as input layer\n",
    "    num_actions : (int) number of nodes of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape, name=\"Layer_1\")) # If a vector the flattening does not have any effect. (only matrices)\n",
    "    model.add(Dense(num_actions,activation=None, name = \"Output\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def simple(input_shape, num_actions):\n",
    "    \"\"\"Feeds into get_model. Sets up a neural network keras model instance.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : (np.array) shape of the state vector to be fed into the model as input layer\n",
    "    num_actions : (int) number of nodes of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, input_shape = input_shape, activation='relu',name = \"Layer_1\"))\n",
    "    model.add(Dense(9, activation='relu', name= \"Layer_2\"))\n",
    "    model.add(Dense(num_actions, activation=None, name= \"Output\"))\n",
    "    return model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "code_folding": [
     237
    ]
   },
   "outputs": [],
   "source": [
    "# ENVIRONMENT CLASS\n",
    "##################################   \n",
    "class Env:\n",
    "    \"\"\"Main class to manage environment. Supplies the environment responses to actions taken.\n",
    "    \n",
    "    Sends commands to sumo to take an action, receives state information,\n",
    "    computes rewards. Does so step by step to allow q-network to train batch-wise.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    net : (str) points to the SUMO .net.xml file which specifies network arquitechture\n",
    "    route : (str) points to the SUMO .rou.xml file which specified traffic demand\n",
    "    use_gui : (bool) Whether to run SUMO simulation with GUI visualisation\n",
    "    time_step : (int) Simulation seconds between actions\n",
    "    sumo_binary : (str) Points to the binary to run sumo\n",
    "    num_actions : (int) number of actions (traffic signal phases)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    start_simulation()\n",
    "        Opens call to sumo\n",
    "    \n",
    "    take_action(action)\n",
    "        Sets the traffic lights according to the action fed as argument\n",
    "\n",
    "    compute_reward(state, next_state)\n",
    "        Takes the current state and next state (from object state) and computes reward\n",
    "\n",
    "    step(action)\n",
    "        Combines take_action, compute_reward and update_state (from observation object)\n",
    "\n",
    "    done()\n",
    "        checks whether all links are empty, and hence the simulation done\n",
    "\n",
    "    stop_simulation()\n",
    "        closes the sumo/traci call\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 net_file, \n",
    "                 route_file,\n",
    "                 state_shape,\n",
    "                 num_actions,\n",
    "                 use_gui = False,\n",
    "                 delta_time=10):\n",
    "        \"\"\"Initialises object instance.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        net_file : (str) SUMO .net.xml file\n",
    "        route_file : (str) SUMO .rou.xml file\n",
    "        state_shape : (np.array) 2-dimensional array specifying state space dimensions\n",
    "        num_actions : (int) specifying the number of actions available\n",
    "        use_gui : (bool) Whether to run SUMO simulation with GUI visualisation\n",
    "        delta_time : (int) Simulation seconds between actions\n",
    "        \"\"\"\n",
    "        \n",
    "        self.net = net_file\n",
    "        self.route = route_file\n",
    "        self.use_gui = use_gui\n",
    "        self.time_step = delta_time\n",
    "        self.input_lanes = [\"4i_0\",\"2i_0\",\"3i_0\",\"1i_0\"] \n",
    "        \n",
    "        if self.use_gui:\n",
    "            self.sumo_binary = sumolib.checkBinary('sumo-gui')\n",
    "        else:\n",
    "            self.sumo_binary = sumolib.checkBinary('sumo')\n",
    "            \n",
    "        self.obs = Observation(state_shape, self.input_lanes)\n",
    "        self.action = Action(num_actions)\n",
    "        \n",
    "        \n",
    "        self.counter = np.zeros((1,2))\n",
    " \n",
    "\n",
    "    def start_simulation(self):\n",
    "        \"\"\"Opens a connection to sumo/traci [with or without GUI] and \n",
    "        updates obs atribute  (the current state of the environment).\n",
    "        \"\"\"\n",
    "    \n",
    "        sumo_cmd = [self.sumo_binary, \n",
    "                    '-n', self.net,\n",
    "                    '-r' ,self.route]\n",
    "        \n",
    "        if self.use_gui:\n",
    "            sumo_cmd.append('--start')\n",
    "        traci.start(sumo_cmd)\n",
    "        self.obs.update_state()\n",
    "        \n",
    "        \n",
    "    def take_action(self,action):\n",
    "        \"\"\"Sets the action variable in sumo/traci to a new value.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : (int) index of the one-hot encoded vector of action to be taken\n",
    "        \"\"\"\n",
    "        \n",
    "        #action = 0 -> row vertically\n",
    "        if action == 0 and traci.trafficlight.getPhase(\"0\") != 0:\n",
    "            traci.trafficlight.setPhase(\"0\",3)\n",
    "            self.counter += self.obs.get()[:,-2:]\n",
    "            self.obs.get()[:,-2:] = 0\n",
    "        #action = 1 -> row horizontally    \n",
    "        elif action == 1 and traci.trafficlight.getPhase(\"0\") != 2:\n",
    "            traci.trafficlight.setPhase(\"0\",1)\n",
    "            self.counter += self.obs.get()[:,-2:]\n",
    "            self.obs.get()[:,-2:] = 0\n",
    "                \n",
    "    def compute_reward(self, state, next_state):\n",
    "        \"\"\" Computes reward from state and next_state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : (np.array) vector of current state\n",
    "        next_state: (np.array) vector of next state\n",
    "        \"\"\"\n",
    "            \n",
    "        # Here is whre reward is specified\n",
    "        a = next_state - state\n",
    "        b = np.round(state,decimals=1)\n",
    "\n",
    "        aux = np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "        \n",
    "        return -np.sum(a) #-np.sum(next_state[0,:4])#np.sum(a[0,4:-1])-np.sum(a[0,:4]) # - delta number of stopped cars\n",
    "        \n",
    "    def compute_waiting_time(self):\n",
    "        aux= []\n",
    "        for lane in self.input_lanes:\n",
    "            aux.append(traci.lane.getWaitingTime(lane))\n",
    "        \n",
    "        return aux         \n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\" Runs one step of the simulation.\n",
    "        \n",
    "        Makes a deep copy of the current state,\n",
    "        runs the simulation with the currently set action,\n",
    "        gets the state after the action,\n",
    "        and computes the reward associated to action taken.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : (int) index of the one-hot encoded vector of action to be taken\n",
    "        \"\"\"\n",
    "        \n",
    "        state = copy.deepcopy(self.obs.get())\n",
    "        wt = self.compute_waiting_time()\n",
    "        \n",
    "        if action != None:\n",
    "            self.take_action(action)\n",
    "        \n",
    "        traci.simulationStep(traci.simulation.getTime() + self.time_step) # Run the simulation time_step (s)\n",
    "        \n",
    "        self.obs.update_state()\n",
    "        next_state = self.obs.get()\n",
    "        \n",
    "        wt_next = self.compute_waiting_time()\n",
    "        reward = self.compute_reward(np.array(wt),np.array(wt_next))\n",
    "        \n",
    "        return state, reward, next_state, self.done()\n",
    "    \n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    def done(self):\n",
    "        \"\"\"Calls sumo/traci to check whether there are still cars in the network\"\"\"\n",
    "        \n",
    "        return traci.simulation.getMinExpectedNumber() == 0\n",
    "    \n",
    "    \n",
    "    def stop_simulation(self):\n",
    "        \"\"\"Closes the sumo/traci connection\"\"\"\n",
    "        \n",
    "        traci.close()\n",
    "        \n",
    "\n",
    "    \n",
    "class Observation:\n",
    "    \"\"\"\n",
    "    Helper class for environment. Handles the updating of the state. Ports the state from sumo/traci to python.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    obs : (np.array) holds the state variables\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    update_state() \n",
    "        reads the state through a call to sumo/traci\n",
    "            --> modify here to include additional state variables, \n",
    "            but make sure to also modify state_shape when running the simulation!\n",
    "            \n",
    "    get()\n",
    "        returns state\n",
    "        \n",
    "    get_reward()\n",
    "        returns reward, computed from current state\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, shape,lanes):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : (tuple) specifying dimensionality of state vector\n",
    "        \"\"\"\n",
    "        \n",
    "        self.obs = np.zeros(shape)\n",
    "        self.lanes = lanes\n",
    "        \n",
    "        \n",
    "    def update_state(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        lanes : (list) hardcoded list of lanes in network\n",
    "        # TODO: read this from network file\n",
    "        \"\"\"\n",
    "                 \n",
    "        for i,lane in enumerate(self.lanes):\n",
    "\n",
    "            self.obs[:,i] = traci.lane.getLastStepOccupancy(lane)\n",
    "            self.obs[:,i+4] = traci.lane.getLastStepMeanSpeed(lane)\n",
    "            \n",
    "        self.obs[:,8] = traci.trafficlight.getPhase(\"0\")\n",
    "        if self.obs[:,8] == 0:\n",
    "            self.obs[:,9] = traci.trafficlight.getPhaseDuration(\"0\") - (traci.trafficlight.getNextSwitch(\"0\") - traci.simulation.getTime())\n",
    "        elif self.obs[:,8] == 2:\n",
    "            self.obs[:,10] = traci.trafficlight.getPhaseDuration(\"0\") - (traci.trafficlight.getNextSwitch(\"0\") - traci.simulation.getTime())\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get(self):\n",
    "        \"\"\"Returns state vector\"\"\"\n",
    "        \n",
    "        return self.obs\n",
    "        \n",
    "        \n",
    "#     def get_reward(self):\n",
    "#         \"\"\"compute reward corresponding to current state\"\"\"\n",
    "        \n",
    "#         return -np.sum(self.obs[0:4])\n",
    "    \n",
    "    \n",
    "    \n",
    "class Action:\n",
    "    \"\"\"\n",
    "    Helper class for observation. One-hot encoding of the phase of the traffic signal. \n",
    "    The methods for this class handle the selection of the best action.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    select_action(policy, **kwargs)\n",
    "        Takes policy as argument, and then calls the corresponding method.\n",
    "        # TODO: assert that the kwargs being fed correspond to the policy selected,\n",
    "        and handle errors\n",
    "\n",
    "    select_rand()\n",
    "        Select one of the actions randomly.\n",
    "    \n",
    "    select_greedy(q-values)\n",
    "        Check which action corresponds to the highest predicted reward/q-value.\n",
    "    \n",
    "    select_epsgreedy(eps, q_values)\n",
    "        Choose whether to explore or exploit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, num_actions):\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.action_space = np.identity(num_actions)\n",
    "        \n",
    "        \n",
    "    def select_action(self, policy, **kwargs):\n",
    "        \"\"\"Takes policy as argument, and then calls the corresponding helper method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        policy : (string) indicating which policy to consider. \n",
    "            Currently implemented: \n",
    "                - Pick action randomly (\"rand\")\n",
    "                - Pick action greedely (\"greedy\")\n",
    "                - Pick action in a eps - greedy fashion (\"epsGreedy\")\n",
    "                                    \n",
    "        **kwargs : arguments for helper methods\n",
    "        \"\"\"\n",
    "        \n",
    "        if policy == \"randUni\":\n",
    "            return self.select_rand()\n",
    "        elif policy == \"greedy\":\n",
    "            return self.select_greedy(**kwargs)\n",
    "        elif policy == \"epsGreedy\":\n",
    "            return self.select_epsgreedy(**kwargs)\n",
    "    \n",
    "    \n",
    "    def select_rand(self):\n",
    "        \"\"\"Feeds into select_greedy or directly into select_action method.\n",
    "        Selects one of the actions randomly.\"\"\"\n",
    "        \n",
    "        return np.random.randint(0, self.num_actions)\n",
    "    \n",
    "    \n",
    "    def select_greedy(self, q_values):\n",
    "        \"\"\"Feeds into select_epsgreedy or directly into select_action method.\n",
    "        Checks which action corresponds to the highest predicted reward/q-value.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        q_values : (np.array) predicted q-values\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    \n",
    "    def select_epsgreedy(self, eps, q_values):\n",
    "        \"\"\"Feeds into select_action method.\n",
    "        If explore, select action randomly,\n",
    "        if exploit, select action greedily using the predicted q values\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        eps : (int) exploration paramter\n",
    "        q_values : (np.array) predicted q-values\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.random.uniform() < eps:\n",
    "            return self.select_rand()\n",
    "        else:\n",
    "            return self.select_greedy(q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# DOUBLE DQN\n",
    "################################\n",
    "\n",
    "SAVE_AFTER = 11000 # Save model checkpoint\n",
    "STORE_LOGS_AFTER = 100\n",
    "\n",
    "class DoubleDQN:\n",
    "    \"\"\"The DQN agent. Handles the updating of q-networks, takes action, and gets environment response.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    q_network : keras model instance to predict q-values for current state\n",
    "    target_q_network : keras model instance to predict q-values for state after action\n",
    "    memory : memory instance - needs to be instantiated first # should this be instantiated here?\n",
    "    gamma : (int) discount factor for rewards\n",
    "    target_update_freq : (int) defines after how many steps the q-network should be re-trained\n",
    "    num_burn_in : (int) defines the size of the replay memory to be filled before, using a specified policy \n",
    "    batch_size : (int) size of batches to be used to train models\n",
    "    trained_episodes : (int) episode counter\n",
    "    max_ep_len : (int) stops simulation after specified number of episodes\n",
    "    output_dir : (str) directory to write tensorboard log and model checkpoints\n",
    "    experiment_id : (str) ID of simulation\n",
    "    summary_writer : tensorboard summary stat writer instance\n",
    "    itr : (int) counts global training steps in all run episodes\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    __compile()\n",
    "        Initialisation method, using the keras instance compile method.\n",
    "        \n",
    "    fill_replay()\n",
    "        Helper method for train. Fills the memory before model training begins.\n",
    "        \n",
    "    predict_q()\n",
    "        Helper method for train. Calls keras predict function using the keras model instance.\n",
    "    \n",
    "    save()\n",
    "        Calls keras save method using the keras model instance.\n",
    "    \n",
    "    update_network(env, policy)\n",
    "        Helper method for train. Computes keras neural network updates using samples from memory.\n",
    "        \n",
    "    train(env, num_episodes, policy, **kwargs)\n",
    "        Main method for the agent. Trains the keras neural network instances, calls all other helper methods.\n",
    "    \n",
    "    evaluate(env)\n",
    "        Use trained agent to run a simulation without training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 q_network, \n",
    "                 target_q_network,\n",
    "                 memory,\n",
    "                 gamma, \n",
    "                 target_update_freq,\n",
    "                 num_burn_in,\n",
    "                 batch_size,\n",
    "                 optimizer,\n",
    "                 loss_func,\n",
    "                 max_ep_length,\n",
    "                 env_name,\n",
    "                 output_dir,\n",
    "                 experiment_id,\n",
    "                 summary_writer,\n",
    "                 model_checkpoint = True,\n",
    "                 opt_metric = None # Used to judge the performance of the ANN\n",
    "                 ): \n",
    "        \"\"\"    \n",
    "        # TODO: specify defaults for required arguments\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        self.q_network = q_network\n",
    "        self.target_q_network = target_q_network\n",
    "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "        self.__compile(optimizer, loss_func,opt_metric)\n",
    "        self.memory = memory\n",
    "        #self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.num_burn_in = num_burn_in\n",
    "        self.batch_size = batch_size\n",
    "        self.trained_episodes = 0\n",
    "        self.max_ep_len = max_ep_length\n",
    "        self.output_dir = output_dir\n",
    "        self.experiment_id = experiment_id\n",
    "        self.summary_writer=summary_writer\n",
    "        self.itr = 0\n",
    "        \n",
    "        #self.learning_type = learning_type\n",
    "        \n",
    "    \n",
    "    def __compile(self, optimizer, loss_func, opt_metric):\n",
    "        \"\"\"Initialisation method, using the keras instance compile method. \"\"\"\n",
    "        \n",
    "        self.q_network.compile(optimizer, loss_func, opt_metric)\n",
    "        self.target_q_network.compile(optimizer, loss_func, opt_metric)\n",
    "        \n",
    "        \n",
    "    def fill_replay(self, env, policy):\n",
    "        \"\"\"Helper method for train. Fills the memory before model training begins.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env :  environment instance\n",
    "        policy : (str) policy to be used to fill memory\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Filling experience replay memory...\")\n",
    "        \n",
    "        env.start_simulation()\n",
    "        \n",
    "        for i in range(self.num_burn_in):\n",
    "            action = env.action.select_action(policy)\n",
    "            state, reward, nextstate, done = env.step(action)\n",
    "            self.memory.append(state, action, reward, nextstate)\n",
    "            \n",
    "        env.stop_simulation()\n",
    "        \n",
    "        print(\"...Done\")\n",
    "    \n",
    "    \n",
    "    def predict_q(self, network, state):\n",
    "        \"\"\"Helper method for train. Calls keras predict function using the keras model instance.\"\"\"\n",
    "        \n",
    "        return network.predict(state)\n",
    "    \n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Calls keras save function using the keras model instance\"\"\"\n",
    "        \n",
    "        filename =  \"{}/model_checkpoints/run{}_iter{}.h5\" .format(self.output_dir, \n",
    "                                               self.experiment_id, \n",
    "                                               self.itr)\n",
    "        self.q_network.save(filename)\n",
    "        \n",
    "        \n",
    "    def named_logs(self, q_network, logs):\n",
    "        \"\"\"create logs\"\"\"\n",
    "        \n",
    "        result = {}\n",
    "        for l in zip(q_network.metrics_names, logs):\n",
    "            result[l[0]] = l[1]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def update_network(self):\n",
    "        \"\"\"Helper method for train. Computes keras neural network updates using samples from memory.\n",
    "        \n",
    "        Notice that we want to incur in loss in the actions that we have selected.\n",
    "        Q_target and Q are set equal for not relevant actions so the loss is 0. \n",
    "        (weights not being updated due to these actions)\"\"\"\n",
    "        \n",
    "        # Sample mini batch\n",
    "        states_m, actions_m, rewards_m, states_m_p = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # attach q-values to states\n",
    "        target_batch = self.q_network.predict(states_m) \n",
    "        target_q = self.target_q_network.predict(states_m_p)\n",
    "        \n",
    "        # choose action\n",
    "        selected_actions = np.argmax(target_q, axis=1)\n",
    "        \n",
    "        # update q-values\n",
    "        for i, action in enumerate(selected_actions):\n",
    "            target_batch[i, action] =  rewards_m[i] + self.gamma * target_q[i, action]\n",
    "        \n",
    "        # keras method to train on batch that returns loss\n",
    "        loss = self.q_network.train_on_batch(states_m, target_batch)\n",
    "        \n",
    "        # get weights\n",
    "        weights = self.q_network.get_weights()\n",
    "        \n",
    "        # Update weights every target_update_freq steps\n",
    "        if self.itr % self.target_update_freq == 0:\n",
    "            self.target_q_network.set_weights(weights)\n",
    "        \n",
    "        # Save network every save_after iterations if monitoring allowed\n",
    "        if self.output_dir and self.itr % SAVE_AFTER == 0:\n",
    "            self.save()\n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def histo_summary(self, values, bins=1000):\n",
    "        \"\"\"Helper function in train method. Log a histogram of the tensor of values for tensorboard.\n",
    "        \n",
    "        Creates a HistogramProto instance that can be fed into Tensorboard.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        values :  (np.array) histogram values\n",
    "        bins : (int) how coarse the histogram is supposed to be\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins = bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "            \n",
    "        return hist\n",
    "         \n",
    "        \n",
    "    def train(self, env, num_episodes, policy, **kwargs):\n",
    "        \"\"\"Main method for the agent. Trains the keras neural network instances, calls all other helper methods.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env: (str) name of environment instance\n",
    "        num_episodes: (int) number of training episodes\n",
    "        policy: (str) name of policy to use to fill memory initially\n",
    "        \"\"\"\n",
    "        \n",
    "        all_stats = []\n",
    "        all_rewards = []\n",
    "        start_train_ep = self.trained_episodes\n",
    "        \n",
    "        for i in range(num_episodes):\n",
    "            # print progress of training\n",
    "            sys.stdout.write(\"\\r\"+'Running episode {} / {}'.format(self.trained_episodes+1, \n",
    "                                                       start_train_ep + num_episodes))\n",
    "            \n",
    "            env.start_simulation()\n",
    "            nextstate = env.obs.get()\n",
    "            done = False\n",
    "            \n",
    "            stats = {\n",
    "                'ep_id' : self.trained_episodes,\n",
    "                'total_reward': 0,\n",
    "                'episode_length': 0,\n",
    "                'max_q_value': 0, \n",
    "            }\n",
    "            \n",
    "            while not done and stats[\"episode_length\"] < self.max_ep_len:\n",
    "                \n",
    "                q_values = self.q_network.predict(nextstate)\n",
    "                action = env.action.select_action(policy, q_values = q_values, eps = eps)\n",
    "                state, reward, nextstate, done = env.step(action)\n",
    "                self.memory.append(state, action, reward, nextstate)\n",
    "                \n",
    "                # Update network weights and record loss for Tensorboard\n",
    "                loss = self.update_network()\n",
    "                \n",
    "                \n",
    "                if self.output_dir and self.itr % STORE_LOGS_AFTER == 0:\n",
    "                    # create list of stats for Tensorboard, add scalars\n",
    "                    summary_list = [tf.Summary.Value(tag = 'loss',\n",
    "                                                      simple_value = loss),\n",
    "                                    tf.Summary.Value(tag = 'Action 1',\n",
    "                                                      simple_value = self.q_network.layers[-1].get_weights()[1][0]),\n",
    "                                    tf.Summary.Value(tag = 'Action 2',\n",
    "                                                      simple_value = self.q_network.layers[-1].get_weights()[1][1]),\n",
    "                                    tf.Summary.Value(tag = 'Episode Length',\n",
    "                                                      simple_value = stats[\"episode_length\"])]\n",
    "\n",
    "                    # add histogram of weights to list of stats for Tensorboard\n",
    "                    for index, layer in enumerate(self.q_network.layers): \n",
    "\n",
    "                        if index != len(self.q_network.layers) - 1:\n",
    "                            summary_list.append(tf.Summary.Value(tag = str(layer.name) + \" weights\" ,\n",
    "                                                            histo = self.histo_summary(layer.get_weights()[0])))\n",
    "                            if len(layer.get_weights()) > 1:\n",
    "                                summary_list.append(tf.Summary.Value(tag = str(layer.name) + \" relu\" ,\n",
    "                                                            histo = self.histo_summary(layer.get_weights()[1])))\n",
    "\n",
    "                        else:\n",
    "                            summary_list.append(tf.Summary.Value(tag = str(layer.name) + \" output weights\" ,\n",
    "                                                            histo = self.histo_summary(layer.get_weights()[0])))\n",
    "                            summary_list.append(tf.Summary.Value(tag = \"output values\",\n",
    "                                                            histo = self.histo_summary(layer.get_weights()[1])))\n",
    "\n",
    "                    # write the list of stats to the logdd\n",
    "                    self.summary_writer.add_summary(tf.Summary(value = summary_list), global_step=self.itr)\n",
    "                \n",
    "                self.itr += 1\n",
    "                \n",
    "                stats[\"ep_id\"] = self.trained_episodes\n",
    "                stats[\"episode_length\"] += 1\n",
    "                stats['total_reward'] += reward\n",
    "                stats['max_q_value'] += max(q_values)\n",
    "            \n",
    "            env.stop_simulation()\n",
    "            self.trained_episodes += 1\n",
    "        \n",
    "            all_stats.append(stats)\n",
    "            all_rewards.append(stats['total_reward'])\n",
    "            \n",
    "        print('\\nCurrent reward mean+std: {} {}'.format(np.mean(stats['total_reward']),np.std(stats['total_reward'])))\n",
    "        return stats\n",
    "        \n",
    "    def evaluate(self, env,static):\n",
    "        \"\"\"Use trained agent to run a simulation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env : environment instance\n",
    "        static : (bool) If the static policy is being evaluated\n",
    "        \"\"\"\n",
    "            \n",
    "        env.start_simulation()\n",
    "        nextstate = env.obs.get()\n",
    "        done = False\n",
    "        it = 0\n",
    "            \n",
    "        while not done and it < self.max_ep_len:\n",
    "            \n",
    "            if not static:\n",
    "                q_values = self.q_network.predict(nextstate)\n",
    "                action = env.action.select_action(\"greedy\", q_values = q_values)\n",
    "                state, reward, nextstate,done = env.step(action)\n",
    "            else:\n",
    "                state, reward, nextstate,done = env.step(None)\n",
    "            it +=1\n",
    "                \n",
    "        env.stop_simulation()\n",
    "        \n",
    "        return it\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling experience replay memory...\n",
      "...Done\n",
      "Running episode 1 / 1\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 2 / 2\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 3 / 3\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 4 / 4\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 5 / 5\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 6 / 6\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 7 / 7\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 8 / 8\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 9 / 9\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 10 / 10\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 11 / 11\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 12 / 12\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 13 / 13\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 14 / 14\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 15 / 15\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 16 / 16\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 17 / 17\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 18 / 18\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 19 / 19\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 20 / 20\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 21 / 21\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 22 / 22\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 23 / 23\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 24 / 24\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 25 / 25\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 26 / 26\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 27 / 27\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 28 / 28\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 29 / 29\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 30 / 30\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 31 / 31\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 32 / 32\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 33 / 33\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 34 / 34\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 35 / 35\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 36 / 36\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 37 / 37\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 38 / 38\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 39 / 39\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 40 / 40\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 41 / 41\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 42 / 42\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 43 / 43\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 44 / 44\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 45 / 45\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 46 / 46\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 47 / 47\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 48 / 48\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 49 / 49\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 50 / 50\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 51 / 51\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 52 / 52\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 53 / 53\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 54 / 54\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 55 / 55\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 56 / 56\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 57 / 57\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 58 / 58\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 59 / 59\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 60 / 60\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 61 / 61\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 62 / 62\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 63 / 63\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 64 / 64\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 65 / 65\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 66 / 66\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 67 / 67\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 68 / 68\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 69 / 69\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 70 / 70\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 71 / 71\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 72 / 72\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 73 / 73\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 74 / 74\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 75 / 75\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 76 / 76\n",
      "Current reward mean+std: 0.0 0.0\n",
      "Running episode 77 / 77"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-589-eab43f1fd5f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0muse_gui\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                )\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mall_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0msumo_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epsGreedy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Imperfect static\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-588-2b7ed1e73a37>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, num_episodes, policy, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# Update network weights and record loss for Tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-588-2b7ed1e73a37>\u001b[0m in \u001b[0;36mupdate_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# attach q-values to states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_q_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_m_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "##################################\n",
    "\n",
    "num_actions = 2\n",
    "state_shape = (1,11) # State var in rows\n",
    "memory_size = 100000\n",
    "gamma = 0.8\n",
    "target_update_frequency = 100\n",
    "num_init_samples_mem = 1000\n",
    "batch_size = 50\n",
    "max_episode_length = 100000\n",
    "optimizer = 'adam'\n",
    "loss = \"mse\"\n",
    "eps = 0.2\n",
    "env_name = \"Simple_Cross\"\n",
    "experiment_id = \"Reward_Waiting_Time\"\n",
    "monitoring = True # Store variables for TensorBoard monitoring and model_checkpoints\n",
    "\n",
    "\n",
    "\n",
    "# Define logs directory if monitoring enabled\n",
    "if monitoring: \n",
    "    output_dir = get_output_folder(\"./Logs\",experiment_id)\n",
    "    summary_writer = tf.summary.FileWriter(logdir=output_dir)\n",
    "else:\n",
    "    output_dir = None\n",
    "    summary_writer = None\n",
    "\n",
    "\n",
    "# Initialize Q-networks (value and target)\n",
    "q_network = get_model('simple',(state_shape[1],),num_actions)\n",
    "target_q_network = get_model('simple',(state_shape[1],),num_actions)\n",
    "\n",
    "# Initialize environment\n",
    "sumo_env =  Env(    \"cross.net.xml\",\n",
    "                    \"cross.rou.xml\",\n",
    "                    state_shape,\n",
    "                    num_actions,\n",
    "                    use_gui=False\n",
    "               )\n",
    "\n",
    "# Initialize replay memory\n",
    "memory = ReplayMemory(    memory_size,\n",
    "                          state_shape,\n",
    "                          num_actions\n",
    "                     )\n",
    "\n",
    "# Initialize Double DQN algorithm\n",
    "ddqn = DoubleDQN(   q_network,\n",
    "                    target_q_network,\n",
    "                    memory,\n",
    "                    gamma,\n",
    "                    target_update_frequency,\n",
    "                    num_init_samples_mem,\n",
    "                    batch_size,\n",
    "                    optimizer,\n",
    "                    loss,\n",
    "                    max_episode_length,\n",
    "                    sumo_env,\n",
    "                    output_dir,\n",
    "                    experiment_id,\n",
    "                    summary_writer\n",
    "                )\n",
    "\n",
    "# Fill Replay Memory\n",
    "ddqn.fill_replay(sumo_env,'rand')\n",
    "\n",
    "# Trains Double DQN\n",
    "num_ep = 200\n",
    "all_stats = []\n",
    "static_it = []\n",
    "static_it_perfect = []\n",
    "for i in range(num_ep):\n",
    "    \n",
    "    generate_routefile()\n",
    "    \n",
    "    # Training\n",
    "    \n",
    "    static = False\n",
    "    sumo_env =  Env(\"cross.net.xml\",\n",
    "                    \"cross.rou.xml\",\n",
    "                    state_shape,\n",
    "                    num_actions,\n",
    "                    use_gui=False\n",
    "               )\n",
    "    all_stats.append(ddqn.train(  sumo_env, 1, \"epsGreedy\", eps=eps))\n",
    "    \n",
    "    # Imperfect static\n",
    "    \n",
    "    sumo_env =  Env(\"cross_no_RL.net.xml\",\n",
    "                \"cross.rou.xml\",\n",
    "                state_shape,\n",
    "                num_actions,\n",
    "                use_gui=False\n",
    "           )\n",
    "      \n",
    "    static = True\n",
    "    static_it.append(ddqn.evaluate(sumo_env,static))\n",
    "    \n",
    "    # Perfect static\n",
    "    sumo_env =  Env(\"cross_no_RL_perfect.net.xml\",\n",
    "                \"cross.rou.xml\",\n",
    "                state_shape,\n",
    "                num_actions,\n",
    "                use_gui=False\n",
    "           )\n",
    "      \n",
    "    static_it_perfect.append(ddqn.evaluate(sumo_env,static))\n",
    "\n",
    "#### How to start a Tensorboard\n",
    "# run the below command in terminal to start Tensorboard, then open http://localhost:6006/ in a browser\n",
    "# !tensorboard --logdir=./Scripts/Tests_Sumo/cross_RL/Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73a21c3940>]"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_stats2 = pd.DataFrame(all_stats)\n",
    "#plt.plot(np.arange(len(all_stats2[\"episode_length\"])),all_stats2[\"episode_length\"])\n",
    "plt.plot(np.arange(len(static_it)),static_it)\n",
    "plt.plot(np.arange(len(static_it_perfect)),static_it_perfect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluates Double DQN\n",
    "sumo_env =  Env(    \"cross.net.xml\",\n",
    "                    \"cross.rou.xml\",\n",
    "                    state_shape,\n",
    "                    num_actions,\n",
    "                    use_gui=True\n",
    "               )\n",
    "#ddqn.evaluate( sumo_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumo_env.start_simulation()\n",
    "nextstate = sumo_env.obs.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.  ,  0.  ,  0.  ,  0.  , 19.44, 19.44, 19.44, 19.44,  0.  ,\n",
       "          4.  ,  0.  ]]),\n",
       " -0.0,\n",
       " array([[ 0.  ,  0.  ,  0.  ,  0.  , 19.44, 19.44, 19.44, 19.44,  2.  ,\n",
       "          0.  ,  4.  ]]),\n",
       " True)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = ddqn.q_network.predict(nextstate)\n",
    "action = sumo_env.action.select_action(\"greedy\", q_values = q_values)\n",
    "sumo_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 372., 2482.]])"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumo_env.counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "801.85px",
    "left": "1673px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
