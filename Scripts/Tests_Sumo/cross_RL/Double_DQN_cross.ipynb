{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "##########################\n",
    "\n",
    "import random\n",
    "import copy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time \n",
    "import os, sys\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Making sure path to SUMO bins correctly specified\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"please declare environment variable 'SUMO_HOME'\")\n",
    "\n",
    "import sumolib\n",
    "import traci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# OUTPUT FILE FOLDER\n",
    "####################################\n",
    "\n",
    "def get_output_folder(parent_dir, exp_id):\n",
    "    \"\"\"Return save folder.\n",
    "    Assumes folders in the parent_dir have suffix -run{run\n",
    "    number}. Finds the highest run number and sets the output folder\n",
    "    to that number + 1. This is just convenient so that if you run the\n",
    "    same script multiple times tensorboard can plot all of the results\n",
    "    on the same plots with different names.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dir: str\n",
    "      Path of the directory containing all experiment runs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    parent_dir/run_dir\n",
    "      Path to this run's save directory.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Returns an error if parent_dir already exists\n",
    "        os.makedirs(parent_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if exp_id in os.listdir(parent_dir):\n",
    "        \n",
    "        experiment_id = 1\n",
    "        new_folder = os.path.join(parent_dir,exp_id+\"_\"+str(experiment_id))\n",
    "        \n",
    "        while os.path.exists(new_folder):\n",
    "            experiment_id +=1\n",
    "            new_folder = os.path.join(parent_dir,exp_id+\"_\"+str(experiment_id))\n",
    "        \n",
    "        parent_dir = new_folder\n",
    "        os.makedirs(parent_dir)\n",
    "        os.mkdir(os.path.join(parent_dir,\"model_checkpoints\"))\n",
    "    else:\n",
    "        parent_dir = os.path.join(parent_dir,exp_id)\n",
    "        os.makedirs(parent_dir)\n",
    "        os.mkdir(os.path.join(parent_dir,\"model_checkpoints\"))\n",
    "        \n",
    "    return parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# MEMORY CLASS\n",
    "##################################################\n",
    "\n",
    "class ReplayMemory():\n",
    "    \"\"\" \n",
    "    Store transitions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    state_shape: np.array shape of state\n",
    "\n",
    "    num_actions: number of actions (traffic signal phases)\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size, state_shape, num_actions):\n",
    "    # initialize the whole memory at once\n",
    "        self.memory = [SingleSample(state_shape,num_actions) for _ in range(max_size)]\n",
    "        self.max_size = max_size\n",
    "        self.itr = 0  # insert the next element here\n",
    "        self.cur_size = 0\n",
    "    \n",
    "    def append(self, state, action, reward, nextstate):\n",
    "        self.memory[self.itr].assign(state, action, reward, nextstate)\n",
    "        self.itr += 1\n",
    "        self.cur_size = min(self.cur_size + 1, self.max_size)\n",
    "        self.itr %= self.max_size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        # Uniform sampling, later prioritized experience replay can be implemented\n",
    "        states, actions,rewards,next_states = [],[],[],[]\n",
    "        for i, idx in enumerate(np.random.randint(0, self.cur_size, size=batch_size)):\n",
    "            transition = self.memory[idx]\n",
    "            states.append(transition.state)\n",
    "            actions.append(transition.action)\n",
    "            rewards.append(transition.reward)\n",
    "            next_states.append(transition.nextstate)\n",
    "        return np.vstack(states), actions, rewards, np.vstack(next_states)\n",
    "    \n",
    "    def print_obs(self,obs):\n",
    "        self.memory[obs].print_obs()\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.cur_size\n",
    "    \n",
    "    \n",
    "class SingleSample():\n",
    "    def __init__(self, state_shape, num_actions): #Num actions not used up to now\n",
    "        self.state = np.zeros(state_shape)\n",
    "        self.action = 0\n",
    "        self.reward = 0\n",
    "        self.nextstate = np.zeros(state_shape)\n",
    "        \n",
    "    def assign(self, state, action, reward, nextstate):\n",
    "        self.state[:] = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.nextstate[:] = nextstate\n",
    "        \n",
    "    def print_obs(self):\n",
    "        print( \"State: \\n\\n\",self.state,\n",
    "               \"\\n\\nAction:\\n\\n\",self.action,\n",
    "               \"\\n\\nReward:\\n\\n\",self.reward,\n",
    "               \"\\n\\nNext State:\\n\\n\",self.nextstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ROUTING (DEMAND)\n",
    "#################################################\n",
    "\n",
    "### TO DO\n",
    "# Create a class to spcify different types of demand\n",
    "\n",
    "def generate_routefile():\n",
    "    random.seed(42)  # make tests reproducible\n",
    "    N = 3600  # number of time steps\n",
    "    # demand per second from different directions\n",
    "\n",
    "    pEW = 1 / 20\n",
    "    pNS = 1 / 80\n",
    "    pWE = 1 / 20\n",
    "    pSN = 1 / 80\n",
    "\n",
    "    with open(\"cross.rou.xml\", \"w\") as routes:\n",
    "        print(\"\"\"<routes>\n",
    "        <vType id=\"car\" accel=\"0.8\" decel=\"4.5\" sigma=\"0.5\" length=\"5\" minGap=\"2.5\" maxSpeed=\"16.67\" guiShape=\"passenger\"/>\n",
    "        <route id=\"right\" edges=\"51o 1i 2o 52i\" />\n",
    "        <route id=\"left\" edges=\"52o 2i 1o 51i\" />\n",
    "        <route id=\"down\" edges=\"54o 4i 3o 53i\" />\n",
    "        <route id=\"up\" edges=\"53o 3i 4o 54i\" />\"\"\", file=routes)\n",
    "        vehNr = 0\n",
    "        for i in range(N):\n",
    "            if random.uniform(0, 1) < pWE:\n",
    "                print('    <vehicle id=\"right_%i\" type=\"car\" route=\"right\" depart=\"%i\" />' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pEW:\n",
    "                print('    <vehicle id=\"left_%i\" type=\"car\" route=\"left\" depart=\"%i\" />' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pNS:\n",
    "                print('    <vehicle id=\"down_%i\" type=\"car\" route=\"up\" depart=\"%i\" color=\"1,0,0\"/>' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "            if random.uniform(0, 1) < pSN:\n",
    "                print('    <vehicle id=\"UP_%i\" type=\"car\" route=\"down\" depart=\"%i\" color=\"1,0,0\"/>' % (\n",
    "                    vehNr, i), file=routes)\n",
    "                vehNr += 1\n",
    "        print(\"</routes>\", file=routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Q NETWORKS\n",
    "################################\n",
    "\n",
    "# More ANN arquitectures to be specified here\n",
    "def get_model(model_name, *args):\n",
    "    if model_name == 'linear':\n",
    "        return linear(*args)\n",
    "    elif model_name == 'simple':\n",
    "        return simple(*args)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "def linear(input_shape, num_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape)) # If a vector this does not have any effect. (only matrices)\n",
    "    model.add(Dense(num_actions,activation=None))\n",
    "    return model\n",
    "\n",
    "def simple(input_shape, num_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, input_shape = input_shape,activation='relu'))\n",
    "    model.add(Dense(9, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation=None))\n",
    "    return model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# ENVIRONMENT CLASS\n",
    "##################################   \n",
    "class Env:\n",
    "    \n",
    "    \"\"\"\n",
    "    SUMO Environment for Traffic Signal Control\n",
    "    net_file: (str) SUMO .net.xml file\n",
    "    route_file: (str) SUMO .rou.xml file\n",
    "    use_gui: (bool) Wheter to run SUMO simulation with GUI visualisation\n",
    "    delta_time: (int) Simulation seconds between actions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 net_file, \n",
    "                 route_file,\n",
    "                 state_shape,\n",
    "                 num_actions,\n",
    "                 use_gui = False,\n",
    "                 delta_time=10):\n",
    "        \n",
    "        self.net = net_file\n",
    "        self.route = route_file\n",
    "        self.use_gui = use_gui\n",
    "        self.time_step = delta_time\n",
    "        if self.use_gui:\n",
    "            self.sumo_binary = sumolib.checkBinary('sumo-gui')\n",
    "        else:\n",
    "            self.sumo_binary = sumolib.checkBinary('sumo')\n",
    "            \n",
    "        self.obs = Observation(state_shape)\n",
    "        self.action = Action(num_actions)\n",
    " \n",
    "    def start_simulation(self):\n",
    "        \n",
    "        sumo_cmd = [self.sumo_binary, \n",
    "                    '-n', self.net,\n",
    "                    '-r' ,self.route]\n",
    "        \n",
    "        if self.use_gui:\n",
    "            sumo_cmd.append('--start')\n",
    "        traci.start(sumo_cmd)\n",
    "        self.obs.update_state()\n",
    "        \n",
    "    def take_action(self,action):\n",
    "        #action = 0 -> row vertically\n",
    "        if action == 0 and traci.trafficlight.getPhase(\"0\") != 0:\n",
    "            traci.trafficlight.setPhase(\"0\",3)\n",
    "        #action = 1 -> row horizontally    \n",
    "        elif action == 1 and traci.trafficlight.getPhase(\"0\") != 2:\n",
    "            traci.trafficlight.setPhase(\"0\",1)\n",
    "                \n",
    "    def compute_reward(self,state,next_state):\n",
    "        # Here is whre reward is specified\n",
    "        a = next_state - state\n",
    "        b = np.round(state,decimals=1)\n",
    "\n",
    "        aux = np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "        \n",
    "        return -np.sum(aux[0,0:4]) #-np.sum(next_state[0,:4])#np.sum(a[0,4:-1])-np.sum(a[0,:4]) # - delta number of stopped cars\n",
    "        \n",
    "    def step( self, action):\n",
    "        \n",
    "        state = copy.deepcopy(self.obs.get())\n",
    "        \n",
    "        if action != None:\n",
    "            self.take_action(action)\n",
    "        \n",
    "        traci.simulationStep(traci.simulation.getTime() + self.time_step) # Run the simulation time_step (s)\n",
    "        \n",
    "        self.obs.update_state()\n",
    "        next_state = self.obs.get()\n",
    "        \n",
    "        reward = self.compute_reward(state,next_state)\n",
    "        \n",
    "        return state, reward, next_state, self.done()\n",
    "    \n",
    "    def done(self):\n",
    "        return traci.simulation.getMinExpectedNumber() == 0\n",
    "    \n",
    "    def stop_simulation(self):\n",
    "        traci.close()\n",
    "        \n",
    "\n",
    "    \n",
    "class Observation:\n",
    "    def __init__(self,\n",
    "                 shape):\n",
    "        self.obs = np.zeros(shape)\n",
    "        \n",
    "    def update_state(self):\n",
    "        \n",
    "        lanes = [\"4i_0\",\"2i_0\",\"3i_0\",\"1i_0\"]           \n",
    "        for i,lane in enumerate(lanes):\n",
    "\n",
    "            self.obs[:,i] = traci.lane.getLastStepHaltingNumber(lane)\n",
    "            self.obs[:,i+4] = traci.lane.getLastStepMeanSpeed(lane)\n",
    "        \n",
    "        self.obs[:,8] = traci.trafficlight.getPhase(\"0\")\n",
    "        \n",
    "    def get(self):\n",
    "        return self.obs\n",
    "        \n",
    "    def get_reward(self):\n",
    "        return -np.sum(self.obs[0:4])\n",
    "    \n",
    "class Action:\n",
    "    \"\"\" One-hot encoding of the phase of the traffic signal\"\"\"\n",
    "    def __init__( self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.action_space = np.identity(num_actions)\n",
    "        \n",
    "    def select_action(self,policy, **kwargs):\n",
    "        if policy == \"randUni\":\n",
    "            return self.select_randuni()\n",
    "        elif policy == \"greedy\":\n",
    "            # NOt implemented yet\n",
    "            return self.select_greedy(**kwargs)\n",
    "        elif policy == \"epsGreedy\":\n",
    "            # Not implemented yet\n",
    "            return self.select_epsgreedy(**kwargs)\n",
    "    \n",
    "    def select_randuni(self):\n",
    "        return np.random.randint(0, self.num_actions)\n",
    "    \n",
    "    def select_greedy(self, q_values):\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def select_epsgreedy(self,eps,q_values):\n",
    "        \n",
    "        if np.random.uniform() < eps:\n",
    "            return self.select_randuni()\n",
    "        else:\n",
    "            return self.select_greedy(q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "code_folding": [
     67,
     159
    ]
   },
   "outputs": [],
   "source": [
    "# DOUBLE DQN\n",
    "################################\n",
    "\n",
    "SAVE_AFTER = 11000 # Save model checkpoint\n",
    "\n",
    "class DoubleDQN:\n",
    "    def __init__(self,\n",
    "                 q_network, \n",
    "                 target_q_network,\n",
    "                 memory, # Replay memory\n",
    "                 gamma, # Discount factor\n",
    "                 target_update_freq, # Frequency to update target network\n",
    "                 num_burn_in, # How many samples to fill replay memory\n",
    "                 #train_freq,\n",
    "                 batch_size,\n",
    "                 optimizer,\n",
    "                 loss_func,\n",
    "                 max_ep_length,\n",
    "                 env_name,\n",
    "                 output_dir,\n",
    "                 experiment_id,\n",
    "                 summary_writer,\n",
    "                 model_checkpoint = True,\n",
    "                 opt_metric=None # Used to judge the performance of the ANN\n",
    "                 ): \n",
    "        self.q_network = q_network\n",
    "        self.target_q_network = target_q_network\n",
    "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "        self.__compile(optimizer, loss_func,opt_metric)\n",
    "        self.memory = memory\n",
    "        #self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.num_burn_in = num_burn_in\n",
    "        #self.train_freq = train_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.training_reward_seen = 0\n",
    "        self.trained_episodes = 0\n",
    "        self.max_ep_len = max_ep_length\n",
    "        self.output_dir = output_dir\n",
    "        self.experiment_id = experiment_id\n",
    "        self.summary_writer=summary_writer\n",
    "        self.itr=0\n",
    "        \n",
    "        #self.learning_type = learning_type\n",
    "        \n",
    "    \n",
    "    def __compile(self, optimizer, loss_func, opt_metric):\n",
    "        \n",
    "        self.q_network.compile(optimizer, loss_func, opt_metric)\n",
    "        self.target_q_network.compile(optimizer, loss_func, opt_metric)\n",
    "        \n",
    "    def fill_replay(self,env,policy):\n",
    "        \n",
    "        print(\"Filling experience replay memory...\")\n",
    "        \n",
    "        env.start_simulation()\n",
    "        \n",
    "        for i in range(self.num_burn_in):\n",
    "            action = env.action.select_action(policy)\n",
    "            state, reward, nextstate,done = env.step(action)\n",
    "            self.memory.append(state,action,reward,nextstate)\n",
    "            \n",
    "        env.stop_simulation()\n",
    "        \n",
    "        print(\"...Done\")\n",
    "    \n",
    "    def predict_q(self,network,state):\n",
    "        return network.predict(state)\n",
    "    \n",
    "    def save(self):\n",
    "        \n",
    "        filename =  \"{}/model_checkpoints/run{}_iter{}.h5\" .format(self.output_dir, \n",
    "                                               self.experiment_id, \n",
    "                                               self.itr)\n",
    "        self.q_network.save(filename)\n",
    "    \n",
    "    def update_network(self):\n",
    "        \n",
    "        # Sample mini batch\n",
    "        states_m,actions_m,rewards_m,states_m_p = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Compute target\n",
    "        # Notice that we want to incur in loss in the actions that we have selected.\n",
    "        # Q_target and Q are set equal for not relevant actions so the loss is 0. \n",
    "        # (weights not being updated due to these actions)\n",
    "        target_batch = self.q_network.predict(states_m) \n",
    "        \n",
    "        target_q = self.target_q_network.predict(states_m_p)\n",
    "        selected_actions = np.argmax(target_q, axis=1 )\n",
    "        \n",
    "        for i,action in enumerate(selected_actions):\n",
    "            \n",
    "            target_batch[i,action] =  rewards_m[i] + self.gamma * np.max(target_q[i])\n",
    "\n",
    "        loss = self.q_network.train_on_batch(states_m,target_batch)\n",
    "        \n",
    "        # Update weights every target_update_freq steps\n",
    "        if self.itr % self.target_update_freq == 0:\n",
    "            self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "        \n",
    "        # Storing Logs\n",
    "        if self.output_dir != None:\n",
    "            # Save network every save_after iterations\n",
    "            if self.itr % SAVE_AFTER == 0:\n",
    "                self.save()\n",
    "                \n",
    "        return loss\n",
    "            \n",
    "    def train(self,env,num_episodes,policy,**kwargs):\n",
    "        \n",
    "        all_stats = []\n",
    "        all_rewards = []\n",
    "        \n",
    "        start_train_ep = self.trained_episodes\n",
    "        \n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            sys.stdout.write(\"\\r\"+'Running episode {} / {}'.format(self.trained_episodes+1, \n",
    "                                                       start_train_ep + num_episodes))\n",
    "            \n",
    "            env.start_simulation()\n",
    "            nextstate = env.obs.get()\n",
    "            done = False\n",
    "            \n",
    "            stats = {\n",
    "                'ep_id' : self.trained_episodes,\n",
    "                'total_reward': 0,\n",
    "                'episode_length': 0,\n",
    "                'max_q_value': 0, \n",
    "            }\n",
    "            \n",
    "            tf_loss = tf.placeholder(tf.float32,shape=None,name='training_loss')\n",
    "            \n",
    "            \n",
    "            while not done and stats[\"episode_length\"] < self.max_ep_len:\n",
    "                \n",
    "                q_values = self.q_network.predict(nextstate)\n",
    "                action = env.action.select_action(policy, q_values = q_values, eps = eps)\n",
    "                state, reward, nextstate,done = env.step(action)\n",
    "                self.memory.append(state,action,reward,nextstate)\n",
    "                \n",
    "                loss = self.update_network()\n",
    "                \n",
    "                self.summary_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag='loss',\n",
    "                                     simple_value=loss.item())]),\n",
    "                    global_step=self.itr)\n",
    "                \n",
    "                self.itr += 1\n",
    "                \n",
    "                stats[\"ep_id\"] = self.trained_episodes\n",
    "                stats[\"episode_length\"] += 1\n",
    "                stats['total_reward'] += reward\n",
    "                stats['max_q_value'] += max(q_values)\n",
    "            \n",
    "            env.stop_simulation()\n",
    "            self.trained_episodes += 1\n",
    "        \n",
    "            all_stats.append(stats)\n",
    "            all_rewards.append(stats['total_reward'])\n",
    "            \n",
    "        print('\\nCurrent mean+std: {} {}'.format(np.mean(all_rewards),np.std(all_rewards)))\n",
    "\n",
    "        \n",
    "    def evaluate(self,env):\n",
    "            \n",
    "            env.start_simulation()\n",
    "            nextstate = env.obs.get()\n",
    "            done = False\n",
    "            it = 0\n",
    "            \n",
    "            while not done and it < self.max_ep_len:\n",
    "                \n",
    "                q_values = self.q_network.predict(nextstate)\n",
    "                action = env.action.select_action(\"greedy\", q_values = q_values)\n",
    "                state, reward, nextstate,done = env.step(action)\n",
    "                it +=1\n",
    "                \n",
    "            env.stop_simulation()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling experience replay memory...\n",
      "...Done\n",
      "Running episode 20 / 20\n",
      "Current mean+std: 4.307559051122688 18.242929332259347\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "##################################\n",
    "\n",
    "num_actions = 2\n",
    "state_shape = (1,9) # State var in rows\n",
    "memory_size = 100000\n",
    "gamma = 0.8\n",
    "target_update_frequency = 100\n",
    "num_init_samples_mem = 1000\n",
    "batch_size = 50\n",
    "max_episode_length = 100000\n",
    "optimizer = 'adam'\n",
    "loss = \"mse\"\n",
    "eps = 0.2\n",
    "env_name = \"Simple_Cross\"\n",
    "experiment_id = \"Test\"\n",
    "monitoring = True # Store variables for TensorBoard monitoring and model_checkpoints\n",
    "\n",
    "\n",
    "\n",
    "# Define logs directory and experiment id. \n",
    "# Experiment id to be changed each time there are structural network changes\n",
    "if monitoring:\n",
    "    output_dir = get_output_folder(\"./Logs\",experiment_id)\n",
    "    summary_writer = tf.summary.FileWriter(logdir=output_dir)\n",
    "else:\n",
    "    output_dir = None\n",
    "    summary_writer = None\n",
    "\n",
    "\n",
    "# Initialize Q-networks (value and target)\n",
    "q_network = get_model('simple',(state_shape[1],),num_actions)\n",
    "target_q_network = get_model('simple',(state_shape[1],),num_actions)\n",
    "\n",
    "# Initialize environment\n",
    "sumo_env =  Env(    \"cross.net.xml\",\n",
    "                    \"cross.rou.xml\",\n",
    "                    state_shape,\n",
    "                    num_actions,\n",
    "                    use_gui=False\n",
    "               )\n",
    "\n",
    "# Initialize replay memory\n",
    "memory = ReplayMemory(    memory_size,\n",
    "                          state_shape,\n",
    "                          num_actions\n",
    "                     )\n",
    "\n",
    "# Initialize Double DQN algorithm\n",
    "ddqn = DoubleDQN(   q_network,\n",
    "                    target_q_network,\n",
    "                    memory,\n",
    "                    gamma,\n",
    "                    target_update_frequency,\n",
    "                    num_init_samples_mem,\n",
    "                    batch_size,\n",
    "                    optimizer,\n",
    "                    loss,\n",
    "                    max_episode_length,\n",
    "                    sumo_env,\n",
    "                    output_dir,\n",
    "                    experiment_id,\n",
    "                    summary_writer\n",
    "                )\n",
    "\n",
    "# Fill Replay Memory\n",
    "ddqn.fill_replay(sumo_env,'randUni')\n",
    "\n",
    "# Trains Double DQN\n",
    "ddqn.train(  sumo_env, 20, \"epsGreedy\", eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluates Double DQN\n",
    "sumo_env =  Env(    \"cross.net.xml\",\n",
    "                    \"cross.rou.xml\",\n",
    "                    state_shape,\n",
    "                    num_actions,\n",
    "                    use_gui=True\n",
    "               )\n",
    "ddqn.evaluate( sumo_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "801.85px",
    "left": "1673px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
